import gzip
import io
import json
import os
import threading
import queue
import re
import time
from datetime import datetime, timedelta
from collections import defaultdict
import ast
from decimal import Decimal
from typing import AsyncIterable, Optional

from fastapi import HTTPException, Request, Depends
from sqlalchemy.orm import Session
from starlette.middleware.base import BaseHTTPMiddleware

from app.auth.database import get_db
from app.auth.dependencies import get_current_user, get_current_user_sync
from app.auth.models import ApiUsageLog, ApiUsageSummary, User
from common.config import API_USAGE_FILE, \
    CLEAR_WEEK, RECORD_API, MAX_ANONYMOUS_SIZE, MAX_USER_SIZE

# === é˜Ÿåˆ— ===
keyword_queue = queue.Queue()
detailed_queue = queue.Queue()


# === APIè°ƒç”¨ç»Ÿè®¡ ===
def update_count(path: str):
    today = datetime.now().strftime("%Y-%m-%d")
    with threading.Lock():
        if not os.path.exists(API_USAGE_FILE):
            with open(API_USAGE_FILE, "w", encoding="utf-8") as f:
                f.write("=== Total Counts ===\n")
                f.write(f"{path}\t1\n")
                f.write("\n=== Daily Counts ===\n")
                f.write(f"{today}\n{path}\t1\n")
            return

        with open(API_USAGE_FILE, "r", encoding="utf-8") as f:
            lines = f.readlines()

        total_counts = defaultdict(int)
        daily_counts = defaultdict(lambda: defaultdict(int))
        section = None
        current_day = None
        for line in lines:
            line = line.strip()
            if line == "=== Total Counts ===":
                section = "total"
                continue
            elif line == "=== Daily Counts ===":
                section = "daily"
                continue
            elif section == "daily" and re.match(r"\d{4}-\d{2}-\d{2}", line):
                current_day = line
                continue
            elif section == "total" and line:
                k, v = line.split("\t")
                total_counts[k] = int(v)
            elif section == "daily" and line:
                k, v = line.split("\t")
                daily_counts[current_day][k] = int(v)

        total_counts[path] += 1
        daily_counts[today][path] += 1

        with open(API_USAGE_FILE, "w", encoding="utf-8") as f:
            f.write("=== Total Counts ===\n")
            for k, v in sorted(total_counts.items()):
                f.write(f"{k}\t{v}\n")
            f.write("\n=== Daily Counts ===\n")
            for date in sorted(daily_counts):
                f.write(f"{date}\n")
                for k, v in sorted(daily_counts[date].items()):
                    f.write(f"{k}\t{v}\n")


def log_detailed_api_to_db(
        db: Session,
        path: str,
        duration: float,
        status_code: int,
        ip: str,
        user_agent: str,
        referer: str,
        user_id: int = None,  # optional
        clear_old: bool = False,  # âœ… æ–°å¢åƒæ•¸ï¼šæ˜¯å¦æ¸…ç†èˆŠè³‡æ–™
        request_size: int = 0,  # âœ… æ–°å¢ï¼šè¯·æ±‚ä½“å¤§å°
        response_size: int = 0  # âœ… æ–°å¢ï¼šå“åº”ä½“å¤§å°
):
    # Step 1: Optional cleanup of old logs
    if clear_old:
        one_week_ago = datetime.utcnow() - timedelta(weeks=1)  # ä¿®æ”¹ä¸º1å‘¨
        db.query(ApiUsageLog).filter(ApiUsageLog.called_at < one_week_ago).delete()
        db.commit()

    # Step 2: Insert detailed log (short-term log)
    log = ApiUsageLog(
        path=path,
        duration=duration,
        status_code=status_code,
        ip=ip,
        user_agent=user_agent,
        referer=referer,
        user_id=user_id,
        request_size=request_size,  # Add request size to the log
        response_size=response_size  # Add response size to the log
    )
    db.add(log)

    # Step 3: Update summary table (long-term accumulated data)
    if user_id:
        summary = db.query(ApiUsageSummary).filter_by(user_id=user_id, path=path).first()
        if summary:
            # ç´¯åŠ è®¡æ•°
            summary.count += 1
            summary.total_duration += Decimal(round(log.duration, 2))
            # ç´¯åŠ ä¸Šè¡Œæµé‡å’Œä¸‹è¡Œæµé‡ï¼ˆå°†å­—èŠ‚è½¬æ¢ä¸º KB å¹¶ä¿ç•™ä¸¤ä½å°æ•°ï¼‰
            summary.total_upload += Decimal(round(log.request_size / 1024, 2))  # è½¬æ¢ä¸º Decimal ç±»å‹
            summary.total_download += Decimal(round(log.response_size / 1024, 2))  # è½¬æ¢ä¸º Decimal ç±»å‹
            # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
            summary.last_updated = datetime.utcnow()
        else:
            summary = ApiUsageSummary(
                user_id=user_id,
                path=path,
                count=1,
                last_updated=datetime.utcnow(),
                # åˆå§‹åŒ–ä¸Šè¡Œæµé‡å’Œä¸‹è¡Œæµé‡ï¼Œè½¬æ¢ä¸º KB å¹¶ä¿ç•™ä¸¤ä½å°æ•°
                total_upload=round(log.request_size / 1024, 2),
                total_download=round(log.response_size / 1024, 2)
            )
            db.add(summary)

        db.commit()

    db.commit()


# app/service/api_logger.py
_workers_started = False
_start_lock = threading.Lock()


# è¿™é‡Œæ˜¯ä¸­é—´ä»¶ï¼Œè´Ÿè´£è®°å½•è¯·æ±‚å’Œå“åº”çš„æµé‡å¤§å°
class TrafficLoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

        if not any(keyword in request.url.path for keyword in RECORD_API):
            return await call_next(request)  # å¦‚æœè·¯å¾„ä¸åŒ…å«ä»»ä½•å…è®¸çš„è¯ï¼Œè·³è¿‡æ—¥å¿—è®°å½•

        # æ‰‹åŠ¨è°ƒç”¨ get_current_user è·å–ç”¨æˆ·
        db = next(get_db())  # ç¡®ä¿æˆ‘ä»¬ä»ç”Ÿæˆå™¨ä¸­è·å–ä¼šè¯å¯¹è±¡
        try:
            user = get_current_user_sync(request, db=db)
        except HTTPException as e:
            if e.status_code == 401:
                user = None  # åŒ¿åä½¿ç”¨è€…ï¼Œå…è¨±ç¹¼çºŒåŸ·è¡Œ
            else:
                raise  # å…¶ä»–éŒ¯èª¤ç…§å¸¸ä¸Ÿå‡º

        # å¦‚æœæ˜¯ GET è¯·æ±‚ï¼Œè®¡ç®— URL å’ŒæŸ¥è¯¢å‚æ•°çš„å¤§å°
        if request.method == "GET":
            url_size = len(request.url.path) + len(request.url.query)  # URL è·¯å¾„ + æŸ¥è¯¢å­—ç¬¦ä¸²çš„é•¿åº¦
            request_size = url_size  # åªè®¡ç®— URL çš„å¤§å°
        else:
            # è·å–è¯·æ±‚ä½“å¤§å°
            request_body = await request.body()
            request_size = len(request_body)

        # è°ƒç”¨ä¸‹æ¸¸åº”ç”¨ï¼ˆè§†å›¾å‡½æ•°ï¼‰
        response = await call_next(request)

        # è®°å½•å“åº”ä½“å¤§å°
        response_body = b""
        async for chunk in response.body_iterator:
            response_body += chunk
        # å¦‚æœå“åº”æ˜¯ JSON æ ¼å¼ï¼Œè¿›è¡Œå‹ç¼©
        if "application/json" in response.headers.get("Content-Type", ""):
            # å‹ç¼© JSON å“åº”ä½“
            # print("å£“ç¸®")
            response_body = compress_json(response_body)
            response.headers["Content-Encoding"] = "gzip"  # è®¾ç½®å“åº”çš„ç¼–ç ç±»å‹
            # æ›´æ–° Content-Length ä¸ºå‹ç¼©åçš„å¤§å°
            response.headers["Content-Length"] = str(len(response_body))

        response_size = len(response_body)
        # if user is None:
        #     print("æœªç™»éŒ„")
        # else :
        #     if user and user.role != "admin":
        #         print("æ™®é€š")
        #     else:
        #         print("ç®¡ç†")
        # åˆ¤æ–­ç”¨æˆ·å’Œå“åº”ä½“å¤§å°æ˜¯å¦ç¬¦åˆé™åˆ¶
        if user is None and response_size > MAX_ANONYMOUS_SIZE:
            raise HTTPException(
                status_code=413,  # Payload Too Large
                detail="ğŸš« ç”±æ–¼æœå‹™å™¨é™åˆ¶ï¼Œæœªç™»éŒ„ç”¨æˆ¶æš«ä¸å…è¨±è«‹æ±‚éå¤šçš„æ•¸æ“š ğŸ™…â€â™‚ï¸ğŸ™…â€â™€ï¸"
            )
        else:
            if user and user.role != "admin" and response_size > MAX_USER_SIZE:
                raise HTTPException(
                    status_code=413,  # Payload Too Large
                    detail="ğŸš« ç”±æ–¼æœå‹™å™¨é™åˆ¶ï¼Œæ‚¨çš„è¿”å›æ•¸æ“šè¶…éé™åˆ¶ï¼Œè«‹æ¸›å°‘è«‹æ±‚ç¯„åœ ğŸ›‘ğŸ’¾"
                )

        # è®¡ç®—å¤„ç†æ—¶é—´
        duration = time.time() - start_time
        log_detailed_api_to_db(
            db=db,
            path=request.url.path,
            duration=duration,
            status_code=response.status_code,
            ip=request.client.host,
            user_agent=request.headers.get("User-Agent"),
            referer=request.headers.get("Referer"),
            user_id=user.id if user else None,
            request_size=request_size,
            response_size=response_size,
            clear_old=CLEAR_WEEK
        )

        # å°†å“åº”ä½“å˜ä¸ºå¼‚æ­¥è¿­ä»£å™¨
        response.body_iterator = iter_response_body(response_body)
        return response


async def iter_response_body(response_body: bytes) -> AsyncIterable[bytes]:
    yield response_body


def compress_json(response_body: bytes) -> bytes:
    # ä½¿ç”¨ gzip å‹ç¼© JSON æ•°æ®
    buf = io.BytesIO()
    with gzip.GzipFile(fileobj=buf, mode='wb') as f:
        f.write(response_body)
    return buf.getvalue()

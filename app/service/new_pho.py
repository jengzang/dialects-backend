import itertools

from app.redis_client import redis_client
from app.service.match_input_tip import match_locations_batch
from app.service.status_arrange_pho import query_characters_by_path, query_by_status, convert_path_str
from common.config import DIALECTS_DB_USER, QUERY_DB_USER
from common.constants import COLUMN_VALUES

import json
import hashlib
import time
import asyncio
from typing import List, Dict, Optional, Any
import redis.asyncio as redis

from common.getloc_by_name_region import query_dialect_abbreviations


def process_chars_status(path_strings, column, combine_query, exclude_columns=None):
    """
    å¤„ç† path_strings å’Œ column çš„ç»„åˆæŸ¥è¯¢é€»è¾‘

    Args:
        exclude_columns: List[str] or None, è¦æŽ’é™¤çš„åˆ—ååˆ—è¡¨
    """
    result = []

    # å¦‚æžœ path_strings æ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼Œåˆ™è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆä¸ºäº†å…¼å®¹å¤šç§ç”¨æ³•ï¼‰
    if isinstance(path_strings, str):
        path_strings = [path_strings]

    if path_strings:
        for path_string in path_strings:
            if combine_query:
                # å¦‚æžœ combine_query ä¸º True, å¤„ç† path_string å’Œ column ç»„åˆå åŠ 
                value_combinations = []
                for col in column:
                    values = COLUMN_VALUES.get(col)
                    if values:
                        value_combinations.append(values)

                # ç”Ÿæˆè·¨åˆ—ç»„åˆï¼Œå¹¶å åŠ  path_string ä¸­çš„æŸ¥è¯¢æ¡ä»¶
                if value_combinations:
                    for value_combination in itertools.product(*value_combinations):
                        # æž„å»ºæ–°çš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼špath_string + æ¯ä¸€åˆ—çš„å€¼
                        query_string = path_string
                        for value, col in zip(value_combination, column):
                            query_string += f"[{value}]{{{col}}}"

                        # æŸ¥è¯¢ç”Ÿæˆçš„ç»„åˆ
                        characters, _ = query_characters_by_path(query_string, exclude_columns=exclude_columns)
                        if characters:
                            display_name = convert_path_str(query_string)
                            result.append({'query': display_name, 'å­—æ•°': len(characters), 'æ±‰å­—': characters})
            else:
                # å¦‚æžœç›´æŽ¥ä¼ å…¥äº† query_stringï¼Œå‰‡ç›´æŽ¥æŸ¥è©¢ä¸¦å°‡çµæžœé™„åŠ åˆ° result ä¸­
                characters, _ = query_characters_by_path(path_string, exclude_columns=exclude_columns)
                if characters:
                    display_name = convert_path_str(path_string)
                    result.append({'query': display_name, 'å­—æ•°': len(characters), 'æ±‰å­—': characters})

    return result


def _run_dialect_analysis_sync(
        char_data_list: List[Dict],
        locations: List[str],
        regions: List[str],
        features: List[str],
        region_mode: str = 'yindian',
        db_path_dialect: str = DIALECTS_DB_USER,
        db_path_query: str = QUERY_DB_USER  # æ–°å¢žï¼šç”¨äºŽæŸ¥è¯¢åœ°ç‚¹çš„æ•°æ®åº“
):
    """
    é€™æ˜¯ sta2pho çš„å¾ŒåŠéƒ¨åˆ†é‚è¼¯é‡å¯«ç‰ˆã€‚
    å®ƒä¸æŸ¥å­—ï¼Œç›´æŽ¥ç”¨ char_data_list è£¡çš„å­—åŽ»æŸ¥æ–¹è¨€ã€‚

    Args:
        db_path_dialect: æ–¹è¨€æ•°æ®åº“è·¯å¾„ï¼ˆç”¨äºŽæŸ¥è¯¢å®žé™…è¯»éŸ³æ•°æ®ï¼‰
        db_path_query: æŸ¥è¯¢æ•°æ®åº“è·¯å¾„ï¼ˆç”¨äºŽæŸ¥è¯¢åœ°ç‚¹ä¿¡æ¯ï¼‰
    """
    # 1. è™•ç†åœ°é»žç°¡ç¨± (è¤‡è£½åŽŸé‚è¼¯)
    locations_new = query_dialect_abbreviations(regions, locations, db_path=db_path_query, region_mode=region_mode)
    match_results = match_locations_batch(" ".join(locations_new))

    # æª¢æŸ¥åŒ¹é…çµæžœ
    if not any(res[1] == 1 for res in match_results):
        # é€™è£¡å¯ä»¥é¸æ“‡æ‹‹å‡ºéŒ¯èª¤æˆ–è¿”å›žç©º
        return []

    unique_abbrs = list({abbr for res in match_results for abbr in res[0]})

    all_results = []

    # 2. éæ­·ç·©å­˜æŸ¥å‡ºä¾†çš„æ¼¢å­—æ•¸æ“š
    for item in char_data_list:
        path_str = item.get('query', 'æœªçŸ¥æ¢ä»¶')
        path_chars = item.get('æ±‰å­—', [])

        if not path_chars:
            continue

        # 3. é‡å°æ¯å€‹ç‰¹å¾µèª¿ç”¨åº•å±¤çš„ query_by_status
        for feature in features:
            # [OK] é€™è£¡ç›´æŽ¥èª¿ç”¨ä½ åŽŸæœ¬çš„åº•å±¤å‡½æ•¸ï¼Œå®Œå…¨ä¸éœ€è¦æ”¹å‹•å®ƒ
            df = query_by_status(
                char_list=path_chars,
                locations=unique_abbrs,
                features=[feature],
                user_input=path_str,  # ç”¨æŸ¥è©¢æ¢ä»¶ä½œç‚ºé¡¯ç¤ºæ¨™ç±¤
                db_path=db_path_dialect
            )

            # å°‡ DataFrame è½‰ç‚ºå­—å…¸æ ¼å¼ä»¥ä¾¿ JSON åºåˆ—åŒ–
            if not df.empty:
                all_results.append(df.to_dict(orient="records"))

    return all_results


# --- 1. ç”Ÿæˆå”¯ä¸€çš„ Cache Key ---
def generate_cache_key(path_strings: Any, column: Any, combine_query: bool, exclude_columns: Any = None) -> str:
    """
    ç”Ÿæˆæ ‡å‡†åŒ–çš„ç¼“å­˜ Key
    æ ¸å¿ƒé€»è¾‘ï¼šæš´åŠ›æ¸…æ´—æ•°æ®ï¼Œå°† None è§†ä¸º []ï¼Œç¡®ä¿ Key çš„å”¯ä¸€æ€§
    """

    # ==========================================
    # ðŸ‘‡ å…³é”®ä¿®æ”¹ï¼šæ•°æ®æ¸…æ´— (Normalization)
    # ==========================================
    # é€»è¾‘ï¼šå¦‚æžœæ˜¯ None (æ²¡ä¼ )ï¼Œæˆ–è€… False (ç©ºåˆ—è¡¨)ï¼Œç»Ÿä¸€å˜æˆ []
    safe_path = path_strings if path_strings else []
    safe_col = column if column else []
    safe_exclude = exclude_columns if exclude_columns else []

    # å¯é€‰ï¼šå¦‚æžœä½ å¸Œæœ› ["A", "B"] å’Œ ["B", "A"] è§†ä¸ºåŒä¸€ä¸ªç¼“å­˜ï¼Œå¯ä»¥æŽ’åº
    # safe_path.sort()
    # safe_col.sort()

    # å¯¹ exclude_columns æŽ’åºä»¥ç¡®ä¿åˆ—è¡¨é¡ºåºä¸å½±å“ç¼“å­˜é”®
    if safe_exclude:
        safe_exclude = sorted(safe_exclude)

    # æž„å»ºç”¨äºŽç”Ÿæˆ Hash çš„å­—å…¸
    key_data = {
        "path": safe_path,
        "col": safe_col,
        "combine": bool(combine_query),  # å¼ºè½¬ boolï¼Œé˜²æ­¢ 0/1 å·®å¼‚
        "exclude": safe_exclude
    }

    # åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²
    # sort_keys=True ä¿è¯å­—å…¸é¡ºåºä¸€è‡´
    # ensure_ascii=False ä¿è¯ä¸­æ–‡å­—ç¬¦ä¸€è‡´
    key_str = json.dumps(key_data, sort_keys=True, ensure_ascii=False)

    # ç”Ÿæˆ MD5
    return "charlist:" + hashlib.md5(key_str.encode('utf-8')).hexdigest()


# ç·©å­˜è®€å– (Async)
async def get_cache(key: str) -> Optional[List[Dict]]:
    try:
        # [OK] åŠ ä¸Š await
        cached_val = await redis_client.get(key)
        if cached_val:
            print(f"ðŸ”¥ [Redis Cache] Hit: {key}")
            return json.loads(cached_val)
    except Exception as e:
        print(f"[X] Redis Read Error: {e}")
    return None


# ç·©å­˜å¯«å…¥ (Async)
async def set_cache(key: str, data: List[Dict], expire_seconds: int = 600):
    try:
        # [OK] åŠ ä¸Š await
        await redis_client.set(key, json.dumps(data), ex=expire_seconds)
        print(f"[SAVE] [Redis Cache] Set: {key}")
    except Exception as e:
        print(f"[X] Redis Write Error: {e}")

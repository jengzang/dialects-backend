import re
from collections import defaultdict

import pandas as pd
from fastapi import HTTPException

from common.path import QUERY_DB_USER, DIALECTS_DB_USER, CHARACTERS_DB_PATH
from app.service.process_sp_input import split_pho_input
from common.constants import AMBIG_VALUES, HIERARCHY_COLUMNS, s2t_column, custom_order
from app.service.getloc_by_name_region import query_dialect_abbreviations
from app.service.match_input_tip import match_locations_batch
from app.sql.db_pool import get_db_pool

# IPA ç¬¦è™Ÿåˆä½µæ˜ å°„è¡¨
MERGE_MAP = {}
MERGE_MAP.update({k: "kÊ·" for k in ["kw", "káµ˜", "káµ›", "kÊ‹", "kÊ·", "kv"]})
MERGE_MAP.update({k: "kÊ°w" for k in ["kÊ·Ê°", "kÊ°Ê·", "kÊ°áµ˜", "kÊ°áµ›", "kÊ°Ê‹", "kÊ‹Ê°", "kÊ°w", "kvÊ°"]})
MERGE_MAP.update({k: "pÊ°Ê‹" for k in ["pÊ°w", "pÊ°áµ˜", "pÊ°Ê‹"]})
MERGE_MAP.update({k: "tÊ°w" for k in ["tÊ°áµ˜", "tÊ°w", "tÊ°Ê‹"]})
MERGE_MAP.update({k: "Ê”" for k in ["(Ê”)", "âˆ…", "Ê”", "Ë€"]})
MERGE_MAP.update({k: "Ê‹" for k in ["v", "Ê‹", "vÊ‹", "w"]})
MERGE_MAP.update({k: "h" for k in ["h", "É¦", "É¦Ê°", "xÊ±", "hÉ¦", "hÊ±", "Ê°"]})
MERGE_MAP.update({k: "hÊ·" for k in ["hÊ·", "hw", "hÊ‹", "É¦Ê‹"]})
MERGE_MAP.update({k: "x" for k in ["x", "xÊ±", "xÉ£", "É£", "Ï‡"]})
MERGE_MAP.update({k: "xÊ·" for k in ["xv", "xÊ‹", "xÊ·", "xáµŠ", "xá¶·"]})
MERGE_MAP.update({k: "d" for k in ["d", "dÌ¥", "É—", "É—w"]})
MERGE_MAP.update({k: "dz" for k in ["dz", "dÌ¥zÌ¥"]})
MERGE_MAP.update({k: "dÊ‘" for k in ["dÊ‘", "dÌ¥Ê‘Ì¥"]})
MERGE_MAP.update({k: "fw" for k in ["fÊ‹", "fw", "fv", "fÊ°", "fÊ±", "f", "ÌŠf"]})
MERGE_MAP.update({k: "l" for k in ["l", "lÌ¥", "lÌ©"]})
MERGE_MAP.update({k: "m" for k in ["m", "mÌ¥", "mÌ©", "mÍ¡b"]})
MERGE_MAP.update({k: "mÊ·" for k in ["mÊ·", "mw", "mÊ‹"]})
MERGE_MAP.update({k: "mÊ°" for k in ["mÊ°", "mÉ¦", "mÊ±"]})
MERGE_MAP.update({k: "sÊ·" for k in ["sw", "sÊ‹", "sÊ·"]})
MERGE_MAP.update({k: "tÊ°" for k in ["tÊ°Ê°", "tÊ±", "tÊ°"]})
MERGE_MAP.update({k: "Å‹Ê·" for k in ["Å‹Ê·", "Å‹w", "Å‹Ê‹"]})
MERGE_MAP.update({k: "Å‹" for k in ["Å‹", "Å‹ÌŠ", "Å‹É¡", "Å‹Í¡É¡", "ng", "nÉ¡"]})
MERGE_MAP.update({k: "É¡" for k in ["É¡", "g", "É¡ÌŠ", "áµ‘É¡"]})
MERGE_MAP.update({k: "b" for k in ["bÌ¥", "É“w", "É“", "áµb", "b", "bv"]})


def custom_phonology_sort(items):
    """
    ä½¿ç”¨ custom_order å°éŸ³éŸ»ç¬¦è™Ÿé€²è¡Œæ’åº
    æ”¯æŒå¤šå­—ç¬¦ IPA ç¬¦è™Ÿï¼ˆå¦‚ pÊ°, tÊ°ï¼‰

    Args:
        items: è¦æ’åºçš„éŸ³éŸ»ç¬¦è™Ÿåˆ—è¡¨

    Returns:
        æ’åºå¾Œçš„åˆ—è¡¨
    """
    def sort_key(item):
        # å…ˆæ‡‰ç”¨ MERGE_MAP æ¨™æº–åŒ–
        normalized = MERGE_MAP.get(item, item)

        # æŒ‰ç…§ custom_order ä¸­çš„ä½ç½®æ’åº
        # å°æ–¼å¤šå­—ç¬¦ç¬¦è™Ÿï¼Œé€å­—ç¬¦åŒ¹é…
        return [
            custom_order.index(normalized[i:i + 2]) if normalized[i:i + 2] in custom_order else
            custom_order.index(normalized[i]) if normalized[i] in custom_order else float('inf')
            for i in range(len(normalized))
        ]

    return sorted(items, key=sort_key)

"""
æ•´é«”æµç¨‹ç¸½çµï¼š

1. ä½¿ç”¨è€…çµ¦å®šåœ°é»ï¼ˆlocationsï¼‰ã€èªéŸ³ç‰¹å¾µï¼ˆfeaturesï¼Œä¾‹å¦‚è²æ¯ã€éŸ»æ¯ï¼‰èˆ‡åˆ†çµ„æ¬„ä½ï¼ˆstatus_inputsï¼Œä¾‹å¦‚"è²çµ„"ï¼‰

2. run_dialect_analysisï¼š
   - è§£æä½¿ç”¨è€…æŒ‡å®šçš„åˆ†çµ„æ¬„ä½ï¼Œå»ºç«‹æ¯å€‹ç‰¹å¾µå°æ‡‰çš„ group_fields
   - èª¿ç”¨ query_dialect_features æŸ¥è©¢æ¯å€‹åœ°é»èˆ‡ç‰¹å¾µå°æ‡‰çš„æ¼¢å­—å­è¡¨ sub_df
   - å°æ¯çµ„æ¼¢å­—èª¿ç”¨ analyze_characters_from_db é€²è¡Œå¯¦éš›åˆ†çµ„èˆ‡çµ±è¨ˆ

3. analyze_characters_from_dbï¼š
   - å¾ characters.db æŸ¥å‡ºæŒ‡å®šæ¼¢å­—çš„èªéŸ³å±¬æ€§
   - æ ¹æ“š group_fields é€²è¡Œåˆ†çµ„
   - è¨ˆç®—å­—æ•¸ã€ä½”æ¯”ã€å¤šåœ°ä½ç°¡è¡¨ï¼Œä¸¦çµ±æ•´ç‚ºçµæœ

4. è¿”å›çš„è³‡æ–™å¯ä»¥ç”¨ä¾†åˆ†æèªéŸ³ç‰¹å¾µåœ¨ä¸åŒåœ°é»çš„åˆ†å¸ƒç‹€æ³èˆ‡éŸ³ç³»ç‰¹é»
"""


def query_dialect_features(locations, features, db_path=DIALECTS_DB_USER, table="dialects"):
    """
    å¾ dialects æ•¸æ“šåº«ä¸­æŸ¥å‡ºæŒ‡å®šåœ°é»èˆ‡ç‰¹å¾µï¼ˆå¦‚è²æ¯ã€éŸ»æ¯ç­‰ï¼‰å°æ‡‰çš„æ¼¢å­—ã€‚

    è¿”å›æ ¼å¼ç‚ºï¼š
    {
        'è²æ¯': {
            'b': {
                'æ¼¢å­—': [...],
                'sub_df': å­è¡¨ DataFrameï¼ˆå«ç°¡ç¨±ã€æ¼¢å­—ã€ç‰¹å¾µå€¼ã€éŸ³ç¯€ã€æ˜¯å¦å¤šéŸ³å­—ï¼‰,
                'å¤šéŸ³å­—è©³æƒ…': [hz1:pron1;pron2, hz2:pron1;pron2]
            },
            ...
        },
        'éŸ»æ¯': {
            ...
        }
    }
    """
    # ä½¿ç”¨é€£æ¥æ± 
    pool = get_db_pool(db_path)
    with pool.get_connection() as conn:
        # å„ªåŒ–ï¼šåªé¸æ“‡éœ€è¦çš„æ¬„ä½ä¸¦æ·»åŠ éæ¿¾æ¢ä»¶
        query = f"""
        SELECT ç°¡ç¨±, æ¼¢å­—, {', '.join(features)}, éŸ³ç¯€, å¤šéŸ³å­—
        FROM {table}
        WHERE ç°¡ç¨± IN ({','.join(f"'{loc}'" for loc in locations)})
        """
        df = pd.read_sql_query(query, conn)

    result = {}

    # é‡å°æ¯å€‹ç‰¹å¾µé€²è¡Œè™•ç†
    for feature in features:
        # åªä¿ç•™è©²ç‰¹å¾µçš„è³‡æ–™ä¸¦ä¸Ÿæ£„ç¼ºå¤±å€¼
        sub_df = df[["ç°¡ç¨±", "æ¼¢å­—", feature, "éŸ³ç¯€", "å¤šéŸ³å­—"]].dropna(subset=[feature])
        feature_dict = {}

        # æŸ¥è©¢æ¯å€‹ç‰¹å¾µå€¼çš„æ‰€æœ‰æ¼¢å­—
        for value in sorted(sub_df[feature].unique()):
            chars = sub_df[sub_df[feature] == value]["æ¼¢å­—"].unique().tolist()
            feature_dict[value] = {
                "æ¼¢å­—": chars,
                "sub_df": sub_df[(sub_df[feature] == value)],
                "å¤šéŸ³å­—è©³æƒ…": []
            }

            # æŸ¥è©¢å¤šéŸ³å­—ï¼šåªæŸ¥è©¢å¤šéŸ³å­—æ¨™è¨»ç‚º "1" çš„è³‡æ–™
            poly_df = sub_df[(sub_df["å¤šéŸ³å­—"] == "1") & (sub_df[feature] == value)]
            poly_dict = {}

            # å„²å­˜è©²ç‰¹å¾µä¸‹çš„æ‰€æœ‰å¤šéŸ³å­—
            for hz in poly_df["æ¼¢å­—"].unique():
                poly_dict[hz] = poly_df[poly_df["æ¼¢å­—"] == hz]["éŸ³ç¯€"].unique().tolist()

            # å­˜å„²å¤šéŸ³å­—è©³æƒ…
            for hz, pron_list in poly_dict.items():
                detail = f"{hz}:{';'.join(pron_list)}"
                feature_dict[value]["å¤šéŸ³å­—è©³æƒ…"].append(detail)

        result[feature] = feature_dict

    return result


def analyze_characters_from_db(
        char_list,
        feature_type,
        feature_value,
        loc,
        sub_df,
        char_db_path=CHARACTERS_DB_PATH,
        group_fields=None,
        exclude_columns=None
):
    """
    æ ¹æ“šæ¼¢å­—åå–®ï¼Œå¾ characters.db ä¸­æŸ¥å‡ºç›¸é—œéŸ³ç³»ç‰¹å¾µè³‡æ–™ï¼Œä¸¦æ ¹æ“šæŒ‡å®šçš„ group_fields æ¬„ä½åˆ†çµ„çµ±è¨ˆã€‚

    åˆ†çµ„å¾Œæ¯çµ„è¿”å›ï¼š
    - è©²çµ„å°æ‡‰çš„å­—ï¼ˆå·²å»é‡ï¼‰
    - å­—æ•¸èˆ‡ä½”æ¯”ï¼ˆä»¥å»é‡å¾Œå­—æ•¸ç‚ºæº–ï¼‰
    - å¤šåœ°ä½è©³æƒ…ï¼ˆä¿ç•™åŸå§‹é‡è¤‡è³‡æ–™ï¼Œç”¨æ–¼å±•ç¤ºï¼‰
    - åˆ†çµ„å€¼ï¼ˆæ¬„ä½å°æ‡‰å€¼ï¼Œä¾‹å¦‚ {'èª¿': 'å¹³', 'æ¸…æ¿': 'å…¨æ¿'}ï¼‰

    è‹¥ group_fields ç‚ºç©ºï¼Œæ ¹æ“šç‰¹å¾µé¡å‹è‡ªå‹•é¸æ“‡é è¨­æ¬„ä½ï¼š
        è²æ¯ âœ æ¯
        éŸ»æ¯ âœ éŸ»
        è²èª¿ âœ æ¸…æ¿ + èª¿

    Args:
        exclude_columns: List[str] or None, ä¾‹å¦‚ ["å¤šåœ°ä½æ¨™è¨˜", "å¤šç­‰"]
                        ç”¨æ–¼éæ¿¾æ‰é€™äº›åˆ—å€¼ç‚º 1ï¼ˆå­—ç¬¦ä¸²æˆ–æ•´æ•¸ï¼‰çš„è¡Œ
    """

    default_grouping = {
        "è²æ¯": ["æ¯"],
        "éŸ»æ¯": ["æ”"],
        "è²èª¿": ["æ¸…æ¿", "èª¿"]
    }
    # print(f"ç‰¹å¾µå€¼{feature_value}")
    if not group_fields:
        group_fields = default_grouping.get(feature_type)
        if not group_fields:
            raise ValueError(f"[X] æœªå®šç¾©çš„ feature_typeï¼š{feature_type}")

    pool = get_db_pool(char_db_path)
    with pool.get_connection() as conn:
        placeholders = ','.join(['?'] * len(char_list))
        query = f"SELECT * FROM characters WHERE æ¼¢å­— IN ({placeholders})"
        df = pd.read_sql_query(query, conn, params=char_list)

    if df.empty:
        return []

    # ã€æ–°å¢ã€‘æ‡‰ç”¨éæ¿¾é‚è¼¯
    if exclude_columns:
        for col_name in exclude_columns:
            if col_name in df.columns:
                # éæ¿¾æ‰è©²åˆ—å€¼ç‚º 1ï¼ˆå­—ç¬¦ä¸²æˆ–æ•´æ•¸ï¼‰çš„è¡Œ
                df = df[
                    (df[col_name] != 1) &
                    (df[col_name] != "1")
                ]

    for col in ["æ”", "éŸ»", "ç­‰", "å‘¼", "å…¥", "æ¸…æ¿", "ç³»", "çµ„", "æ¯", "èª¿", "éƒ¨ä½", "æ–¹å¼", "å¤šåœ°ä½æ¨™è¨˜"]:
        if col not in df.columns:
            df[col] = None

    total_chars = len(set(sub_df["æ¼¢å­—"]))
    grouped_result = []

    df = df.dropna(subset=group_fields)
    grouped = df.groupby(group_fields)

    for group_keys, group_df in grouped:
        # ç‰¹å®šæ¬„ä½éœ€è¦å¾Œç¶´
        suffix_map = {
            "ç³»": "ç³»",
            "çµ„": "çµ„",
            "æ¯": "æ¯",
            "æ”": "æ”",
            "éŸ»": "éŸ»"
        }

        # ä½¿ç”¨ group_df ä¸­ç¬¬ä¸€ç­†è³‡æ–™å–å¾—æ¬„ä½å€¼ï¼ˆè‹¥éœ€è¦ç”¨åˆ° rowï¼Œå¯å–æ¨£ä¸€ç­†ï¼‰
        _, sample_row = next(group_df.iterrows())

        # å»ºæ§‹ valueï¼ˆåŠ å¾Œç¶´ï¼‰
        value_parts = []
        for field, val in zip(group_fields, group_keys):
            if val in AMBIG_VALUES:
                suffix = suffix_map.get(field)
                if suffix:
                    val = f"{val}{suffix}"
            value_parts.append(val)
        group_value = "Â·".join(value_parts)

        # æœ€çµ‚çš„åˆ†çµ„å€¼æ ¼å¼
        # group_values = {group_key_label: group_value}
        group_values = {feature_value: group_value}

        # ä»¥ä¸‹åŸæœ¬çš„é‚è¼¯ç…§èˆŠ
        unique_chars = group_df["æ¼¢å­—"].unique().tolist()
        count = len(unique_chars)

        poly_details = []
        poly_chars = group_df[group_df["å¤šåœ°ä½æ¨™è¨˜"] == "1"]["æ¼¢å­—"].unique()
        for hz in poly_chars:
            sub = df[(df["æ¼¢å­—"] == hz) & (df["å¤šåœ°ä½æ¨™è¨˜"] == "1")]
            summary = []
            for _, row in sub.iterrows():
                parts = f"{row['æ”']}{row['å‘¼']}{row['ç­‰']}{row['éŸ»']}{row['èª¿']}"
                meta = f"{row['éƒ¨ä½']}Â·{row['æ–¹å¼']}Â·{row['æ¯']}"
                summary.append(f"{parts},{meta}")
            poly_details.append(f"{hz}: {' | '.join(summary)}")
        # print(f"ğŸ§© ç•¶å‰åˆ†æåœ°é»ï¼š{loc}")
        # print(f"ğŸ”¢ total_chars for {loc}: {total_chars}")
        # print(f"ğŸ“„ ç‰¹å¾µ {group_value} çš„å­—æ•¸ï¼š{count}")

        grouped_result.append({
            "åœ°é»": loc,
            "ç‰¹å¾µé¡åˆ¥": feature_type,
            "ç‰¹å¾µå€¼": feature_value,
            "åˆ†çµ„å€¼": group_values,
            "å­—æ•¸": count,
            "ä½”æ¯”": round(count / total_chars, 4) if total_chars else 0,
            "å°æ‡‰å­—": unique_chars,
            "å¤šåœ°ä½è©³æƒ…": "; ".join(poly_details)
        })

    return pd.DataFrame(grouped_result)


def pho2sta(locations, regions, features, status_inputs,
            pho_values=None,
            dialect_db_path=DIALECTS_DB_USER,
            character_db_path=CHARACTERS_DB_PATH, region_mode='yindian',
            exclude_columns=None,
            query_db_path=QUERY_DB_USER):  # æ–°å¢ï¼šç”¨äºæŸ¥è¯¢åœ°ç‚¹çš„æ•°æ®åº“
    def convert_simplified_to_traditional(simplified_text):
        return "".join([s2t_column.get(ch, ch) for ch in simplified_text])

    pho_values = split_pho_input(pho_values or [])

    grouping_columns_map = {}
    for idx, feature in enumerate(features):
        user_input = status_inputs[idx] if idx < len(status_inputs) else ""

        # [OK] æœ€é–‹å§‹å°±åšç°¡é«”è½‰ç¹é«”è½‰æ›
        user_input = convert_simplified_to_traditional(user_input)

        # å˜—è©¦åŒ¹é…æ¬„ä½
        user_columns = [col for col in HIERARCHY_COLUMNS if col in user_input]

        if user_columns:
            print(f"[OK] ç‰¹å¾µã€{feature}ã€‘ä½¿ç”¨åˆ†çµ„æ¬„ä½ï¼š{user_columns}")
            grouping_columns_map[feature] = user_columns
        else:
            print(f"[X] è¼¸å…¥ã€Œ{user_input}ã€æœªåŒ¹é…ä»»ä½•æ¬„ä½ï¼Œç‰¹å¾µã€{feature}ã€‘å°‡ä½¿ç”¨é è¨­åˆ†çµ„æ¬„ä½")
            grouping_columns_map[feature] = None

    locations_new = query_dialect_abbreviations(regions, locations, db_path=query_db_path, region_mode=region_mode)
    match_results = match_locations_batch(" ".join(locations_new))
    if not any(res[1] == 1 for res in match_results):
        # print("ğŸ›‘ æ²’æœ‰ä»»ä½•åœ°é»å®Œå…¨åŒ¹é…ï¼Œçµ‚æ­¢åˆ†æã€‚")
        raise HTTPException(status_code=400, detail="ğŸ›‘ æ²’æœ‰ä»»ä½•åœ°é»å®Œå…¨åŒ¹é…ï¼Œçµ‚æ­¢åˆ†æã€‚")
        # return []

    unique_abbrs = list({abbr for res in match_results for abbr in res[0]})
    # print(f"\nğŸ“ ç¢ºèªåŒ¹é…åœ°é»ï¼š{unique_abbrs}")

    results = []
    dialect_output = query_dialect_features(unique_abbrs, features, db_path=dialect_db_path)

    for loc in unique_abbrs:
        # print(f"\nğŸ”· é–‹å§‹è™•ç†åœ°é»ï¼š{loc}")
        for feature in features:
            # print(f"  â”œâ”€â”€ ç‰¹å¾µï¼š{feature}")
            group_fields = grouping_columns_map.get(feature)

            feature_items = dialect_output[feature].items()

            # éæ¿¾ pho_valuesï¼ˆè‹¥æœ‰ï¼‰
            if pho_values:
                print(pho_values)
                filtered_items = []
                for fv, d in feature_items:
                    # æª¢æŸ¥ pho_values ä¸­çš„æ¯å€‹å…ƒç´ 
                    match_found = False
                    for pho_value in pho_values:
                        # å¦‚æœ pho_value å«æœ‰æ¼¢å­—ï¼Œå‰‡é€²è¡Œæ¨¡ç³ŠåŒ¹é…
                        if any('\u4e00' <= char <= '\u9fff' for char in pho_value):  # æª¢æŸ¥æ˜¯å¦åŒ…å«æ¼¢å­—
                            if re.search(pho_value, fv):  # æ¨¡ç³ŠåŒ¹é…
                                match_found = True
                                break
                        else:
                            # å¦‚æœæ²’æœ‰æ¼¢å­—ï¼Œå‰‡é€²è¡Œå®Œå…¨åŒ¹é…
                            if fv == pho_value:
                                match_found = True
                                break
                    if match_found:
                        filtered_items.append((fv, d))

                if filtered_items:
                    # print(f"     ğŸ“Œ éæ¿¾ç‰¹å¾µå€¼ï¼š{[fv for fv, _ in filtered_items]}")
                    feature_items = filtered_items
                else:
                    print("     [!] ç„¡åŒ¹é…ç‰¹å¾µå€¼ï¼Œfallback ä½¿ç”¨å…¨éƒ¨")

            for feature_value, data in feature_items:
                sub_df = data["sub_df"]
                loc_chars = sub_df[sub_df["ç°¡ç¨±"] == loc]["æ¼¢å­—"].unique().tolist()
                # print(f"     â¤ é‹ç®—ç‰¹å¾µå€¼ï¼š{feature_value}ï¼ˆå­—æ•¸ï¼š{len(loc_chars)}ï¼‰")

                if not loc_chars:
                    # print("        [!] è©²ç‰¹å¾µå€¼åœ¨æ­¤åœ°é»ç„¡è³‡æ–™ï¼Œç•¥é")
                    continue

                result = analyze_characters_from_db(
                    char_list=loc_chars,
                    feature_type=feature,
                    feature_value=feature_value,
                    loc=loc,
                    sub_df=sub_df[sub_df["ç°¡ç¨±"] == loc],
                    char_db_path=character_db_path,
                    group_fields=group_fields,
                    exclude_columns=exclude_columns
                )

                results.extend(result if isinstance(result, list) else [result])

    return results


def get_feature_counts(locations, db_path=DIALECTS_DB_USER, table="dialects"):
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ UNION ALL å°†ä¸‰æ¬¡è¡¨æ‰«æåˆå¹¶ä¸ºä¸€æ¬¡æŸ¥è¯¢
    æ˜¾è‘—æå‡æŸ¥è¯¢æ€§èƒ½ï¼ˆ3æ¬¡æ‰«æ â†’ 1æ¬¡æ‰«æï¼‰
    """
    result = defaultdict(lambda: defaultdict(dict))

    pool = get_db_pool(db_path)
    with pool.get_connection() as conn:
        cursor = conn.cursor()

        # [OK] ä¼˜åŒ–ï¼šä½¿ç”¨ UNION ALL åˆå¹¶ä¸‰ä¸ªæŸ¥è¯¢ä¸ºä¸€æ¬¡è¡¨æ‰«æ
        placeholders = ','.join(['?' for _ in locations])

        query_combined = f"""
            SELECT ç°¡ç¨±, 'è²æ¯' as feature_type, è²æ¯ as value, COUNT(DISTINCT æ¼¢å­—) AS å­—æ•¸
            FROM {table}
            WHERE ç°¡ç¨± IN ({placeholders})
            GROUP BY ç°¡ç¨±, è²æ¯

            UNION ALL

            SELECT ç°¡ç¨±, 'éŸ»æ¯' as feature_type, éŸ»æ¯ as value, COUNT(DISTINCT æ¼¢å­—) AS å­—æ•¸
            FROM {table}
            WHERE ç°¡ç¨± IN ({placeholders})
            GROUP BY ç°¡ç¨±, éŸ»æ¯

            UNION ALL

            SELECT ç°¡ç¨±, 'è²èª¿' as feature_type, è²èª¿ as value, COUNT(DISTINCT æ¼¢å­—) AS å­—æ•¸
            FROM {table}
            WHERE ç°¡ç¨± IN ({placeholders})
            GROUP BY ç°¡ç¨±, è²èª¿
        """

        # æ‰§è¡Œåˆå¹¶åçš„æŸ¥è¯¢ï¼ˆå‚æ•°éœ€è¦é‡å¤3æ¬¡ï¼Œå¯¹åº”3ä¸ªWHEREå­å¥ï¼‰
        cursor.execute(query_combined, locations * 3)

        # å¤„ç†æ‰€æœ‰ç»“æœï¼ŒæŒ‰ç‰¹å¾ç±»å‹åˆ†ç¦»
        all_rows = cursor.fetchall()
        for row in all_rows:
            loc = row[0]
            feature_type = row[1]
            value = row[2]
            count = row[3]

            # æ ¹æ®ç‰¹å¾ç±»å‹å¡«å……ç»“æœå­—å…¸
            result[loc][feature_type][value] = count

    return result


def get_all_phonology_matrices(locations=None, db_path=DIALECTS_DB_USER, table="dialects"):
    """
    è·å–æŒ‡å®šåœ°ç‚¹çš„å£°æ¯-éŸµæ¯-æ±‰å­—äº¤å‰è¡¨æ•°æ®

    Args:
        locations: åœ°ç‚¹åˆ—è¡¨ï¼Œå¦‚æœä¸º None åˆ™è·å–æ‰€æœ‰åœ°ç‚¹
        db_path: æ•°æ®åº“è·¯å¾„
        table: è¡¨å

    Returns:
        {
            "locations": List[str],
            "data": {
                "åœ°ç‚¹1": {
                    "initials": List[str],
                    "finals": List[str],
                    "tones": List[str],
                    "matrix": Dict[str, Dict[str, Dict[str, List[str]]]]
                },
                ...
            }
        }
    """
    # æŒ‰åœ°ç‚¹åˆ†ç»„çš„æ•°æ®
    locations_data = defaultdict(lambda: {
        "matrix": defaultdict(lambda: defaultdict(lambda: defaultdict(list))),
        "initials": set(),
        "finals": set(),
        "tones": set()
    })

    pool = get_db_pool(db_path)
    with pool.get_connection() as conn:
        cursor = conn.cursor()

        # æ„å»ºæŸ¥è¯¢è¯­å¥
        if locations and len(locations) > 0:
            # æŸ¥è¯¢æŒ‡å®šåœ°ç‚¹
            placeholders = ','.join(f"'{loc}'" for loc in locations)
            query = f"""
                SELECT ç°¡ç¨±, è²æ¯, éŸ»æ¯, è²èª¿, æ¼¢å­—
                FROM {table}
                WHERE ç°¡ç¨± IN ({placeholders})
                ORDER BY ç°¡ç¨±, è²æ¯, éŸ»æ¯, è²èª¿
            """
        else:
            # æŸ¥è¯¢æ‰€æœ‰åœ°ç‚¹
            query = f"""
                SELECT ç°¡ç¨±, è²æ¯, éŸ»æ¯, è²èª¿, æ¼¢å­—
                FROM {table}
                ORDER BY ç°¡ç¨±, è²æ¯, éŸ»æ¯, è²èª¿
            """

        cursor.execute(query)
        rows = cursor.fetchall()

        # å¤„ç†æŸ¥è¯¢ç»“æœ
        for row in rows:
            location = row[0]  # åœ°ç‚¹
            initial = row[1]   # å£°æ¯
            final = row[2]     # éŸµæ¯
            tone = row[3]      # å£°è°ƒ
            char = row[4]      # æ±‰å­—

            # è·³è¿‡ç©ºå€¼
            if not location or not initial or not final or not tone or not char:
                continue

            # æ·»åŠ åˆ°è¯¥åœ°ç‚¹çš„çŸ©é˜µ
            loc_data = locations_data[location]
            loc_data["matrix"][initial][final][tone].append(char)

            # æ”¶é›†è¯¥åœ°ç‚¹çš„å”¯ä¸€å€¼
            loc_data["initials"].add(initial)
            loc_data["finals"].add(final)
            loc_data["tones"].add(tone)

    # è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼
    result = {
        "locations": sorted(list(locations_data.keys())),
        "data": {}
    }

    for location, loc_data in locations_data.items():
        result["data"][location] = {
            "initials": custom_phonology_sort(list(loc_data["initials"])),
            "finals": custom_phonology_sort(list(loc_data["finals"])),
            "tones": custom_phonology_sort(list(loc_data["tones"])),
            "matrix": {
                initial: {
                    final: dict(tones_dict)
                    for final, tones_dict in finals_dict.items()
                }
                for initial, finals_dict in loc_data["matrix"].items()
            }
        }

    return result

# if __name__ == "__main__":
#     pd.set_option('display.max_rows', None)
#     pd.set_option('display.max_columns', None)
#     pd.set_option('display.max_colwidth', None)
#     pd.set_option('display.width', 0)
#     locations = ['é«˜å·æ³—æ°´ é«˜å·æ ¹å­']
#     # features = ['è²æ¯', 'éŸ»æ¯', 'è²èª¿']
#     features = ['éŸ»æ¯']
#     # group_inputs = ['çµ„', 'æ”ç­‰', 'æ¸…æ¿èª¿']  # [OK] ç”¨æˆ¶æŒ‡å®šåˆ†çµ„æ¬„ä½
#     group_inputs = ['æ”']  # [OK] ç”¨æˆ¶æŒ‡å®šåˆ†çµ„æ¬„ä½
#     pho_value = ['l', 'm', 'an']
#     regions = ['å°ç¶', 'å„‹å·']
#     results = pho2sta(locations, regions, features, group_inputs, pho_value)
#
#     for row in results:
#         print(row)

# location = ['æ±èèåŸ', 'é›²æµ®å¯Œæ—']
# result = get_feature_counts(location)
# for loc, features in result.items():
#     print(f"åœ°ç‚¹: {loc}")
#     for feature, values in features.items():
#         print(f"  {feature}:")
#         for value, count in values.items():
#             print(f"    {value}: {count} å­—")
#     print("\n")
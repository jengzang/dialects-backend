import re
import sqlite3

import pandas as pd
from fastapi import HTTPException

from common.config import CHARACTERS_DB_PATH, DIALECTS_DB_USER
from app.service.process_sp_input import split_pho_input
from common.constants import AMBIG_VALUES, HIERARCHY_COLUMNS, s2t_column
from common.getloc_by_name_region import query_dialect_abbreviations
from app.service.match_input_tip import match_locations_batch

"""
æ•´é«”æµç¨‹ç¸½çµï¼š

1. ä½¿ç”¨è€…çµ¦å®šåœ°é»ï¼ˆlocationsï¼‰ã€èªéŸ³ç‰¹å¾µï¼ˆfeaturesï¼Œä¾‹å¦‚è²æ¯ã€éŸ»æ¯ï¼‰èˆ‡åˆ†çµ„æ¬„ä½ï¼ˆstatus_inputsï¼Œä¾‹å¦‚"è²çµ„"ï¼‰

2. run_dialect_analysisï¼š
   - è§£æä½¿ç”¨è€…æŒ‡å®šçš„åˆ†çµ„æ¬„ä½ï¼Œå»ºç«‹æ¯å€‹ç‰¹å¾µå°æ‡‰çš„ group_fields
   - èª¿ç”¨ query_dialect_features æŸ¥è©¢æ¯å€‹åœ°é»èˆ‡ç‰¹å¾µå°æ‡‰çš„æ¼¢å­—å­è¡¨ sub_df
   - å°æ¯çµ„æ¼¢å­—èª¿ç”¨ analyze_characters_from_db é€²è¡Œå¯¦éš›åˆ†çµ„èˆ‡çµ±è¨ˆ

3. analyze_characters_from_dbï¼š
   - å¾ characters.db æŸ¥å‡ºæŒ‡å®šæ¼¢å­—çš„èªéŸ³å±¬æ€§
   - æ ¹æ“š group_fields é€²è¡Œåˆ†çµ„
   - è¨ˆç®—å­—æ•¸ã€ä½”æ¯”ã€å¤šåœ°ä½ç°¡è¡¨ï¼Œä¸¦çµ±æ•´ç‚ºçµæœ

4. è¿”å›çš„è³‡æ–™å¯ä»¥ç”¨ä¾†åˆ†æèªéŸ³ç‰¹å¾µåœ¨ä¸åŒåœ°é»çš„åˆ†å¸ƒç‹€æ³èˆ‡éŸ³ç³»ç‰¹é»
"""


def query_dialect_features(locations, features, db_path=DIALECTS_DB_USER, table="dialects"):
    """
    å¾ dialects æ•¸æ“šåº«ä¸­æŸ¥å‡ºæŒ‡å®šåœ°é»èˆ‡ç‰¹å¾µï¼ˆå¦‚è²æ¯ã€éŸ»æ¯ç­‰ï¼‰å°æ‡‰çš„æ¼¢å­—ã€‚

    è¿”å›æ ¼å¼ç‚ºï¼š
    {
        'è²æ¯': {
            'b': {
                'æ¼¢å­—': [...],
                'sub_df': å­è¡¨ DataFrameï¼ˆå«ç°¡ç¨±ã€æ¼¢å­—ã€ç‰¹å¾µå€¼ã€éŸ³ç¯€ã€æ˜¯å¦å¤šéŸ³å­—ï¼‰,
                'å¤šéŸ³å­—è©³æƒ…': [hz1:pron1;pron2, hz2:pron1;pron2]
            },
            ...
        },
        'éŸ»æ¯': {
            ...
        }
    }
    """
    # é€£æ¥è³‡æ–™åº«
    conn = sqlite3.connect(db_path)

    # å„ªåŒ–ï¼šåªé¸æ“‡éœ€è¦çš„æ¬„ä½ä¸¦æ·»åŠ éæ¿¾æ¢ä»¶
    query = f"""
    SELECT ç°¡ç¨±, æ¼¢å­—, {', '.join(features)}, éŸ³ç¯€, å¤šéŸ³å­—
    FROM {table}
    WHERE ç°¡ç¨± IN ({','.join(f"'{loc}'" for loc in locations)})
    """
    df = pd.read_sql_query(query, conn)
    conn.close()

    result = {}

    # é‡å°æ¯å€‹ç‰¹å¾µé€²è¡Œè™•ç†
    for feature in features:
        # åªä¿ç•™è©²ç‰¹å¾µçš„è³‡æ–™ä¸¦ä¸Ÿæ£„ç¼ºå¤±å€¼
        sub_df = df[["ç°¡ç¨±", "æ¼¢å­—", feature, "éŸ³ç¯€", "å¤šéŸ³å­—"]].dropna(subset=[feature])
        feature_dict = {}

        # æŸ¥è©¢æ¯å€‹ç‰¹å¾µå€¼çš„æ‰€æœ‰æ¼¢å­—
        for value in sorted(sub_df[feature].unique()):
            chars = sub_df[sub_df[feature] == value]["æ¼¢å­—"].unique().tolist()
            feature_dict[value] = {
                "æ¼¢å­—": chars,
                "sub_df": sub_df[(sub_df[feature] == value)],
                "å¤šéŸ³å­—è©³æƒ…": []
            }

            # æŸ¥è©¢å¤šéŸ³å­—ï¼šåªæŸ¥è©¢å¤šéŸ³å­—æ¨™è¨»ç‚º "1" çš„è³‡æ–™
            poly_df = sub_df[(sub_df["å¤šéŸ³å­—"] == "1") & (sub_df[feature] == value)]
            poly_dict = {}

            # å„²å­˜è©²ç‰¹å¾µä¸‹çš„æ‰€æœ‰å¤šéŸ³å­—
            for hz in poly_df["æ¼¢å­—"].unique():
                poly_dict[hz] = poly_df[poly_df["æ¼¢å­—"] == hz]["éŸ³ç¯€"].unique().tolist()

            # å­˜å„²å¤šéŸ³å­—è©³æƒ…
            for hz, pron_list in poly_dict.items():
                detail = f"{hz}:{';'.join(pron_list)}"
                feature_dict[value]["å¤šéŸ³å­—è©³æƒ…"].append(detail)

        result[feature] = feature_dict

    return result


def analyze_characters_from_db(
        char_list,
        feature_type,
        feature_value,
        loc,
        sub_df,
        char_db_path=CHARACTERS_DB_PATH,
        group_fields=None
):
    """
    æ ¹æ“šæ¼¢å­—åå–®ï¼Œå¾ characters.db ä¸­æŸ¥å‡ºç›¸é—œéŸ³ç³»ç‰¹å¾µè³‡æ–™ï¼Œä¸¦æ ¹æ“šæŒ‡å®šçš„ group_fields æ¬„ä½åˆ†çµ„çµ±è¨ˆã€‚

    åˆ†çµ„å¾Œæ¯çµ„è¿”å›ï¼š
    - è©²çµ„å°æ‡‰çš„å­—ï¼ˆå·²å»é‡ï¼‰
    - å­—æ•¸èˆ‡ä½”æ¯”ï¼ˆä»¥å»é‡å¾Œå­—æ•¸ç‚ºæº–ï¼‰
    - å¤šåœ°ä½è©³æƒ…ï¼ˆä¿ç•™åŸå§‹é‡è¤‡è³‡æ–™ï¼Œç”¨æ–¼å±•ç¤ºï¼‰
    - åˆ†çµ„å€¼ï¼ˆæ¬„ä½å°æ‡‰å€¼ï¼Œä¾‹å¦‚ {'èª¿': 'å¹³', 'æ¸…æ¿': 'å…¨æ¿'}ï¼‰

    è‹¥ group_fields ç‚ºç©ºï¼Œæ ¹æ“šç‰¹å¾µé¡å‹è‡ªå‹•é¸æ“‡é è¨­æ¬„ä½ï¼š
        è²æ¯ âœ æ¯
        éŸ»æ¯ âœ éŸ»
        è²èª¿ âœ æ¸…æ¿ + èª¿
    """

    default_grouping = {
        "è²æ¯": ["æ¯"],
        "éŸ»æ¯": ["æ”"],
        "è²èª¿": ["æ¸…æ¿", "èª¿"]
    }
    # print(f"ç‰¹å¾µå€¼{feature_value}")
    if not group_fields:
        group_fields = default_grouping.get(feature_type)
        if not group_fields:
            raise ValueError(f"âŒ æœªå®šç¾©çš„ feature_typeï¼š{feature_type}")

    conn = sqlite3.connect(char_db_path)
    placeholders = ','.join(['?'] * len(char_list))
    query = f"SELECT * FROM characters WHERE æ¼¢å­— IN ({placeholders})"
    df = pd.read_sql_query(query, conn, params=char_list)
    conn.close()

    for col in ["æ”", "å‘¼", "ç­‰", "éŸ»", "èª¿", "ç³»", "çµ„", "æ¯", "å¤šåœ°ä½æ¨™è¨˜"]:
        if col not in df.columns:
            df[col] = None

    total_chars = len(set(sub_df["æ¼¢å­—"]))
    grouped_result = []

    df = df.dropna(subset=group_fields)
    grouped = df.groupby(group_fields)

    for group_keys, group_df in grouped:
        # ç‰¹å®šæ¬„ä½éœ€è¦å¾Œç¶´
        suffix_map = {
            "ç³»": "ç³»",
            "çµ„": "çµ„",
            "æ¯": "æ¯",
            "æ”": "æ”",
            "éŸ»": "éŸ»"
        }

        # ä½¿ç”¨ group_df ä¸­ç¬¬ä¸€ç­†è³‡æ–™å–å¾—æ¬„ä½å€¼ï¼ˆè‹¥éœ€è¦ç”¨åˆ° rowï¼Œå¯å–æ¨£ä¸€ç­†ï¼‰
        _, sample_row = next(group_df.iterrows())

        # å»ºæ§‹ valueï¼ˆåŠ å¾Œç¶´ï¼‰
        value_parts = []
        for field, val in zip(group_fields, group_keys):
            if val in AMBIG_VALUES:
                suffix = suffix_map.get(field)
                if suffix:
                    val = f"{val}{suffix}"
            value_parts.append(val)
        group_value = "Â·".join(value_parts)

        # æœ€çµ‚çš„åˆ†çµ„å€¼æ ¼å¼
        # group_values = {group_key_label: group_value}
        group_values = {feature_value: group_value}

        # ä»¥ä¸‹åŸæœ¬çš„é‚è¼¯ç…§èˆŠ
        unique_chars = group_df["æ¼¢å­—"].unique().tolist()
        count = len(unique_chars)

        poly_details = []
        poly_chars = group_df[group_df["å¤šåœ°ä½æ¨™è¨˜"] == "1"]["æ¼¢å­—"].unique()
        for hz in poly_chars:
            sub = df[(df["æ¼¢å­—"] == hz) & (df["å¤šåœ°ä½æ¨™è¨˜"] == "1")]
            summary = []
            for _, row in sub.iterrows():
                parts = f"{row['æ”']}{row['å‘¼']}{row['ç­‰']}{row['éŸ»']}{row['èª¿']}"
                meta = f"{row['ç³»']}Â·{row['çµ„']}Â·{row['æ¯']}"
                summary.append(f"{parts},{meta}")
            poly_details.append(f"{hz}: {' | '.join(summary)}")
        # print(f"ğŸ§© ç•¶å‰åˆ†æåœ°é»ï¼š{loc}")
        # print(f"ğŸ”¢ total_chars for {loc}: {total_chars}")
        # print(f"ğŸ“„ ç‰¹å¾µ {group_value} çš„å­—æ•¸ï¼š{count}")

        grouped_result.append({
            "åœ°é»": loc,
            "ç‰¹å¾µé¡åˆ¥": feature_type,
            "ç‰¹å¾µå€¼": feature_value,
            "åˆ†çµ„å€¼": group_values,
            "å­—æ•¸": count,
            "ä½”æ¯”": round(count / total_chars, 4) if total_chars else 0,
            "å°æ‡‰å­—": unique_chars,
            "å¤šåœ°ä½è©³æƒ…": "; ".join(poly_details)
        })

    return pd.DataFrame(grouped_result)


def pho2sta(locations, regions, features, status_inputs,
            pho_values=None,
            dialect_db_path=DIALECTS_DB_USER,
            character_db_path=CHARACTERS_DB_PATH, region_mode='yindian'):
    def convert_simplified_to_traditional(simplified_text):
        return "".join([s2t_column.get(ch, ch) for ch in simplified_text])

    pho_values = split_pho_input(pho_values or [])

    grouping_columns_map = {}
    for idx, feature in enumerate(features):
        user_input = status_inputs[idx] if idx < len(status_inputs) else ""

        # âœ… æœ€é–‹å§‹å°±åšç°¡é«”è½‰ç¹é«”è½‰æ›
        user_input = convert_simplified_to_traditional(user_input)

        # å˜—è©¦åŒ¹é…æ¬„ä½
        user_columns = [col for col in HIERARCHY_COLUMNS if col in user_input]

        if user_columns:
            print(f"âœ… ç‰¹å¾µã€{feature}ã€‘ä½¿ç”¨åˆ†çµ„æ¬„ä½ï¼š{user_columns}")
            grouping_columns_map[feature] = user_columns
        else:
            print(f"âŒ è¼¸å…¥ã€Œ{user_input}ã€æœªåŒ¹é…ä»»ä½•æ¬„ä½ï¼Œç‰¹å¾µã€{feature}ã€‘å°‡ä½¿ç”¨é è¨­åˆ†çµ„æ¬„ä½")
            grouping_columns_map[feature] = None

    locations_new = query_dialect_abbreviations(regions, locations,region_mode=region_mode)
    match_results = match_locations_batch(" ".join(locations_new))
    if not any(res[1] == 1 for res in match_results):
        # print("ğŸ›‘ æ²’æœ‰ä»»ä½•åœ°é»å®Œå…¨åŒ¹é…ï¼Œçµ‚æ­¢åˆ†æã€‚")
        raise HTTPException(status_code=404, detail="ğŸ›‘ æ²’æœ‰ä»»ä½•åœ°é»å®Œå…¨åŒ¹é…ï¼Œçµ‚æ­¢åˆ†æã€‚")
        # return []

    unique_abbrs = list({abbr for res in match_results for abbr in res[0]})
    # print(f"\nğŸ“ ç¢ºèªåŒ¹é…åœ°é»ï¼š{unique_abbrs}")

    results = []
    dialect_output = query_dialect_features(unique_abbrs, features, db_path=dialect_db_path)

    for loc in unique_abbrs:
        # print(f"\nğŸ”· é–‹å§‹è™•ç†åœ°é»ï¼š{loc}")
        for feature in features:
            # print(f"  â”œâ”€â”€ ç‰¹å¾µï¼š{feature}")
            group_fields = grouping_columns_map.get(feature)

            feature_items = dialect_output[feature].items()

            # éæ¿¾ pho_valuesï¼ˆè‹¥æœ‰ï¼‰
            if pho_values:
                print(pho_values)
                filtered_items = []
                for fv, d in feature_items:
                    # æª¢æŸ¥ pho_values ä¸­çš„æ¯å€‹å…ƒç´ 
                    match_found = False
                    for pho_value in pho_values:
                        # å¦‚æœ pho_value å«æœ‰æ¼¢å­—ï¼Œå‰‡é€²è¡Œæ¨¡ç³ŠåŒ¹é…
                        if any('\u4e00' <= char <= '\u9fff' for char in pho_value):  # æª¢æŸ¥æ˜¯å¦åŒ…å«æ¼¢å­—
                            if re.search(pho_value, fv):  # æ¨¡ç³ŠåŒ¹é…
                                match_found = True
                                break
                        else:
                            # å¦‚æœæ²’æœ‰æ¼¢å­—ï¼Œå‰‡é€²è¡Œå®Œå…¨åŒ¹é…
                            if fv == pho_value:
                                match_found = True
                                break
                    if match_found:
                        filtered_items.append((fv, d))

                if filtered_items:
                    # print(f"     ğŸ“Œ éæ¿¾ç‰¹å¾µå€¼ï¼š{[fv for fv, _ in filtered_items]}")
                    feature_items = filtered_items
                else:
                    print("     âš ï¸ ç„¡åŒ¹é…ç‰¹å¾µå€¼ï¼Œfallback ä½¿ç”¨å…¨éƒ¨")

            for feature_value, data in feature_items:
                sub_df = data["sub_df"]
                loc_chars = sub_df[sub_df["ç°¡ç¨±"] == loc]["æ¼¢å­—"].unique().tolist()
                # print(f"     â¤ é‹ç®—ç‰¹å¾µå€¼ï¼š{feature_value}ï¼ˆå­—æ•¸ï¼š{len(loc_chars)}ï¼‰")

                if not loc_chars:
                    # print("        âš ï¸ è©²ç‰¹å¾µå€¼åœ¨æ­¤åœ°é»ç„¡è³‡æ–™ï¼Œç•¥é")
                    continue

                result = analyze_characters_from_db(
                    char_list=loc_chars,
                    feature_type=feature,
                    feature_value=feature_value,
                    loc=loc,
                    sub_df=sub_df[sub_df["ç°¡ç¨±"] == loc],
                    char_db_path=character_db_path,
                    group_fields=group_fields,
                )

                results.extend(result if isinstance(result, list) else [result])

    return results

# if __name__ == "__main__":
#     pd.set_option('display.max_rows', None)
#     pd.set_option('display.max_columns', None)
#     pd.set_option('display.max_colwidth', None)
#     pd.set_option('display.width', 0)
#     locations = ['é«˜å·æ³—æ°´ é«˜å·æ ¹å­']
#     # features = ['è²æ¯', 'éŸ»æ¯', 'è²èª¿']
#     features = ['éŸ»æ¯']
#     # group_inputs = ['çµ„', 'æ”ç­‰', 'æ¸…æ¿èª¿']  # âœ… ç”¨æˆ¶æŒ‡å®šåˆ†çµ„æ¬„ä½
#     group_inputs = ['æ”']  # âœ… ç”¨æˆ¶æŒ‡å®šåˆ†çµ„æ¬„ä½
#     pho_value = ['l', 'm', 'an']
#     regions = ['å°ç¶', 'å„‹å·']
#     results = pho2sta(locations, regions, features, group_inputs, pho_value)
#
#     for row in results:
#         print(row)

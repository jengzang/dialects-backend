import asyncio
# import gzip
# import io
# import json
# import os
import threading
import multiprocessing  # [FIX] æ”¹ç”¨è·¨è¿›ç¨‹é˜Ÿåˆ—
# import re
import time
from datetime import datetime, timedelta
# from collections import defaultdict
# import ast
from decimal import Decimal
# from typing import AsyncIterable, Optional

from fastapi import HTTPException, Request, Depends
from sqlalchemy.orm import Session
# from sqlalchemy import text
from starlette.middleware.base import BaseHTTPMiddleware
from queue import Empty  # åªå¼•å…¥é€™å€‹ç•°å¸¸é¡ï¼Œä¸å¼•å…¥ Queue é¡
from app.auth.database import get_db
from app.auth.dependencies import get_current_user, get_current_user_for_middleware
from app.auth.models import ApiUsageLog, ApiUsageSummary, User
from app.logs.database import SessionLocal as LogsSessionLocal
from app.logs.models import ApiKeywordLog, ApiStatistics, ApiVisitLog
from common.config import KEYWORD_LOG_FILE, SUMMARY_FILE, API_USAGE_FILE, API_DETAILED_JSON, API_DETAILED_FILE, \
    CLEAR_WEEK, RECORD_API, MAX_ANONYMOUS_SIZE, MAX_USER_SIZE, BATCH_SIZE, SIZE_THRESHOLD, IGNORE_API

# === é˜Ÿåˆ—ï¼ˆè·¨è¿›ç¨‹ï¼‰ ===
# [FIX] æ”¹ç”¨ multiprocessing.Queue ä»¥æ”¯æŒä¸»è¿›ç¨‹ä¸­çš„åå°çº¿ç¨‹
# keyword_queue = queue.Queue()  # [X] ä¸å†ä½¿ç”¨ txtæ–‡ä»¶é˜Ÿåˆ—
log_queue = multiprocessing.Queue(maxsize=2000)  # [OK] ApiUsageLog é˜Ÿåˆ—ï¼ˆauth.dbï¼‰- é™åˆ¶ 2000 æ¡
keyword_log_queue = multiprocessing.Queue(maxsize=5000)  # [OK] ApiKeywordLog é˜Ÿåˆ—ï¼ˆlogs.dbï¼‰- é™åˆ¶ 5000 æ¡
statistics_queue = multiprocessing.Queue(maxsize=1000)  # [OK] ApiStatistics é˜Ÿåˆ—ï¼ˆlogs.dbï¼‰- é™åˆ¶ 1000 æ¡
html_visit_queue = multiprocessing.Queue(maxsize=500)  # [OK] HTML é¡µé¢è®¿é—®ç»Ÿè®¡é˜Ÿåˆ—ï¼ˆlogs.dbï¼‰- é™åˆ¶ 500 æ¡
summary_queue = multiprocessing.Queue(maxsize=1000)  # [NEW] ApiUsageSummary é˜Ÿåˆ—ï¼ˆauth.dbï¼‰- é™åˆ¶ 1000 æ¡


# === å…³é”®è¯æ—¥å¿—ï¼ˆå†™å…¥logs.dbï¼‰ ===
def log_keyword(path: str, field: str, value):
    """è®°å½• API è°ƒç”¨çš„å‚æ•°å…³é”®è¯åˆ°æ•°æ®åº“"""
    timestamp = datetime.now()
    log = ApiKeywordLog(
        timestamp=timestamp,
        path=path,
        field=field,
        value=str(value)
    )
    keyword_log_queue.put(log)


def log_all_fields(path: str, param_dict: dict):
    """è®°å½• API è°ƒç”¨çš„æ‰€æœ‰å‚æ•°"""
    for field, value in param_dict.items():
        if value is not None and value != [] and value != "":
            log_keyword(path, field, value)


# ===å…³é”®è¯æ—¥å¿—å†™å…¥çº¿ç¨‹ï¼ˆlogs.dbï¼‰===
def keyword_log_writer():
    """åå°çº¿ç¨‹ï¼šæ‰¹é‡å†™å…¥ ApiKeywordLog åˆ° logs.db"""
    batch = []
    batch_size = 50  # æ¯ 50 æ¡æ‰¹é‡å†™å…¥ä¸€æ¬¡

    while True:
        try:
            # ç­‰å¾…é˜Ÿåˆ—ä¸­çš„æ•°æ®ï¼Œè¶…æ—¶1ç§’
            item = keyword_log_queue.get(timeout=1)
            if item is None:  # åœæ­¢ä¿¡å·
                break

            batch.append(item)

            # æ‰¹é‡å†™å…¥
            if len(batch) >= batch_size:
                db = LogsSessionLocal()
                try:
                    db.bulk_save_objects(batch)
                    db.commit()
                    batch = []
                except Exception as e:
                    print(f"[X] å†™å…¥å…³é”®è¯æ—¥å¿—å¤±è´¥: {e}")
                    db.rollback()
                finally:
                    db.close()

        except Empty:
            # é˜Ÿåˆ—ç©ºæ—¶ï¼Œå†™å…¥å‰©ä½™æ•°æ®
            if batch:
                db = LogsSessionLocal()
                try:
                    db.bulk_save_objects(batch)
                    db.commit()
                    batch = []
                except Exception as e:
                    print(f"[X] å†™å…¥å…³é”®è¯æ—¥å¿—å¤±è´¥: {e}")
                    db.rollback()
                finally:
                    db.close()
        except Exception as e:
            print(f"[X] å…³é”®è¯æ—¥å¿—çº¿ç¨‹é”™è¯¯: {e}")

    # çº¿ç¨‹ç»“æŸå‰ï¼Œå†™å…¥å‰©ä½™æ•°æ®
    if batch:
        db = LogsSessionLocal()
        try:
            db.bulk_save_objects(batch)
            db.commit()
        except Exception as e:
            print(f"[X] å†™å…¥å…³é”®è¯æ—¥å¿—å¤±è´¥: {e}")
            db.rollback()
        finally:
            db.close()


# === APIç»Ÿè®¡æ›´æ–°çº¿ç¨‹ï¼ˆlogs.dbï¼‰===
def statistics_writer():
    """åå°çº¿ç¨‹ï¼šæ‰¹é‡æ›´æ–° ApiStatistics"""
    batch = []
    batch_size = 50
    batch_timeout = 120.0

    while True:
        try:
            item = statistics_queue.get(timeout=batch_timeout)
            if item is None:  # åœæ­¢ä¿¡å·
                break

            batch.append(item)

            # æ‰¹æ¬¡æ»¡æ—¶å†™å…¥
            if len(batch) >= batch_size:
                _process_statistics_batch(batch)
                batch = []

        except Empty:
            # è¶…æ—¶æ—¶å†™å…¥å‰©ä½™é¡¹
            if batch:
                _process_statistics_batch(batch)
                batch = []
        except Exception as e:
            print(f"[X] statistics_writer é”™è¯¯: {e}")

    # çº¿ç¨‹ç»“æŸå‰å†™å…¥å‰©ä½™æ•°æ®
    if batch:
        _process_statistics_batch(batch)


def _process_statistics_batch(batch: list):
    """æ‰¹é‡å¤„ç†ç»Ÿè®¡æ›´æ–°"""
    db = LogsSessionLocal()
    try:
        for path, date_obj in batch:
            # æ›´æ–°æ€»è®¡
            update_statistic(db, "usage_total", None, "path", path)

            # æ›´æ–°æ¯æ—¥ç»Ÿè®¡
            if date_obj:
                update_statistic(db, "usage_daily", date_obj, "path", path)

        db.commit()
        print(f"[OK] æ‰¹é‡æ›´æ–° {len(batch)} æ¡ç»Ÿè®¡")
    except Exception as e:
        print(f"[X] ç»Ÿè®¡æ‰¹æ¬¡å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


def update_statistic(db: Session, stat_type: str, date: datetime, category: str, item: str):
    """æ›´æ–°æˆ–åˆ›å»ºç»Ÿè®¡è®°å½•ï¼ˆä½¿ç”¨ UPSERT é¿å…å¹¶å‘é—®é¢˜ï¼‰"""
    from sqlalchemy import text

    # ä½¿ç”¨ SQLite çš„ INSERT OR IGNORE + UPDATE ç­–ç•¥
    try:
        # å…ˆå°è¯•æ›´æ–°
        result = db.execute(
            text("""
                UPDATE api_statistics
                SET count = count + 1, updated_at = datetime('now')
                WHERE stat_type = :stat_type
                  AND category = :category
                  AND item = :item
                  AND (
                    (date IS NULL AND :date IS NULL) OR
                    (date = :date)
                  )
            """),
            {
                "stat_type": stat_type,
                "date": date,
                "category": category,
                "item": item
            }
        )

        # å¦‚æœæ²¡æœ‰æ›´æ–°ä»»ä½•è¡Œï¼ˆè®°å½•ä¸å­˜åœ¨ï¼‰ï¼Œåˆ™æ’å…¥æ–°è®°å½•
        if result.rowcount == 0:
            db.execute(
                text("""
                    INSERT OR IGNORE INTO api_statistics (stat_type, date, category, item, count, updated_at)
                    VALUES (:stat_type, :date, :category, :item, 1, datetime('now'))
                """),
                {
                    "stat_type": stat_type,
                    "date": date,
                    "category": category,
                    "item": item
                }
            )
    except Exception as e:
        print(f"[X] æ›´æ–°ç»Ÿè®¡å¤±è´¥: stat_type={stat_type}, category={category}, item={item}, error={e}")
        raise



# === APIè°ƒç”¨ç»Ÿè®¡ï¼ˆå†™å…¥logs.dbï¼‰===
def update_count(path: str):
    """æ›´æ–° API è°ƒç”¨æ¬¡æ•°ç»Ÿè®¡"""
    today = datetime.now()
    statistics_queue.put((path, today))


def update_html_visit(path: str):
    """æ›´æ–° HTML é¡µé¢è®¿é—®æ¬¡æ•°ç»Ÿè®¡"""
    today = datetime.now()
    html_visit_queue.put((path, today))


# === HTML é¡µé¢è®¿é—®ç»Ÿè®¡å†™å…¥çº¿ç¨‹ï¼ˆlogs.dbï¼‰===
def html_visit_writer():
    """åå°çº¿ç¨‹ï¼šæ‰¹é‡æ›´æ–° HTML é¡µé¢è®¿é—®ç»Ÿè®¡"""
    batch = []
    batch_size = 3
    batch_timeout = 5.0

    while True:
        try:
            item = html_visit_queue.get(timeout=batch_timeout)
            if item is None:  # åœæ­¢ä¿¡å·
                break

            batch.append(item)

            # æ‰¹æ¬¡æ»¡æ—¶å†™å…¥
            if len(batch) >= batch_size:
                _process_html_visit_batch(batch)
                batch = []

        except Empty:
            # è¶…æ—¶æ—¶å†™å…¥å‰©ä½™é¡¹
            if batch:
                _process_html_visit_batch(batch)
                batch = []
        except Exception as e:
            print(f"[X] html_visit_writer é”™è¯¯: {e}")

    # çº¿ç¨‹ç»“æŸå‰å†™å…¥å‰©ä½™æ•°æ®
    if batch:
        _process_html_visit_batch(batch)


def _process_html_visit_batch(batch: list):
    """æ‰¹é‡å¤„ç† HTML è®¿é—®ç»Ÿè®¡"""
    db = LogsSessionLocal()
    try:
        for path, date_obj in batch:
            # æ›´æ–°æ€»è®¡
            update_html_visit_stat(db, path, None)

            # æ›´æ–°æ¯æ—¥ç»Ÿè®¡
            update_html_visit_stat(db, path, date_obj.date())

        db.commit()
        print(f"[OK] æ‰¹é‡æ›´æ–° {len(batch)} æ¡ HTML è®¿é—®ç»Ÿè®¡")
    except Exception as e:
        print(f"[X] HTML è®¿é—®ç»Ÿè®¡æ‰¹æ¬¡å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


def update_html_visit_stat(db: Session, path: str, date):
    """æ›´æ–°æˆ–åˆ›å»º HTML é¡µé¢è®¿é—®ç»Ÿè®¡è®°å½•ï¼ˆä½¿ç”¨ UPSERT é¿å…å¹¶å‘é—®é¢˜ï¼‰"""
    from sqlalchemy import text

    # ä½¿ç”¨ SQLite çš„ INSERT OR REPLACE è¯­æ³•ï¼ˆUPSERTï¼‰
    # è¿™æ ·å¯ä»¥é¿å… rollback å½±å“æ•´ä¸ª session
    try:
        # å…ˆå°è¯•æ›´æ–°
        result = db.execute(
            text("""
                UPDATE api_visit_log
                SET count = count + 1, updated_at = datetime('now')
                WHERE path = :path AND (
                    (date IS NULL AND :date IS NULL) OR
                    (date = :date)
                )
            """),
            {"path": path, "date": date}
        )

        # å¦‚æœæ²¡æœ‰æ›´æ–°ä»»ä½•è¡Œï¼ˆè®°å½•ä¸å­˜åœ¨ï¼‰ï¼Œåˆ™æ’å…¥æ–°è®°å½•
        if result.rowcount == 0:
            db.execute(
                text("""
                    INSERT OR IGNORE INTO api_visit_log (path, date, count, updated_at)
                    VALUES (:path, :date, 1, datetime('now'))
                """),
                {"path": path, "date": date}
            )
    except Exception as e:
        print(f"[X] æ›´æ–° HTML è®¿é—®ç»Ÿè®¡å¤±è´¥: path={path}, date={date}, error={e}")
        raise



def log_writer_thread(db: Session):
    """æ‰¹é‡å†™å…¥ ApiUsageLog åˆ° auth.db"""
    batch = []
    batch_size = 50
    batch_timeout = 120.0

    while True:
        try:
            # ä½¿ç”¨è¶…æ—¶ç­‰å¾…ï¼Œè€Œé sleep
            item = log_queue.get(timeout=batch_timeout)

            if item is None:  # åœæ­¢ä¿¡å·
                break

            batch.append(item)

            # æ‰¹æ¬¡æ»¡æ—¶å†™å…¥
            if len(batch) >= batch_size:
                _write_log_batch(db, batch)
                batch = []

        except Empty:
            # è¶…æ—¶æ—¶å†™å…¥å‰©ä½™é¡¹
            if batch:
                _write_log_batch(db, batch)
                batch = []
        except Exception as e:
            print(f"[X] log_writer_thread é”™è¯¯: {e}")

    # çº¿ç¨‹ç»“æŸå‰å†™å…¥å‰©ä½™æ•°æ®
    if batch:
        _write_log_batch(db, batch)


def _write_log_batch(db: Session, batch: list):
    """æ‰¹é‡å†™å…¥æ—¥å¿—"""
    try:
        db.bulk_save_objects(batch)
        db.commit()
        print(f"[OK] æ‰¹é‡å†™å…¥ {len(batch)} æ¡æ—¥å¿—")
    except Exception as e:
        print(f"[X] æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
        db.rollback()


# å¼‚æ­¥å†™å…¥æ—¥å¿—çš„å‡½æ•°
def log_detailed_api_to_db(
        db: Session,
        path: str,
        duration: float,
        status_code: int,
        ip: str,
        user_agent: str,
        referer: str,
        user_id: int = None,
        clear_old: bool = False,
        request_size: int = 0,
        response_size: int = 0,
        start_time: float = None  # start_time å‚æ•°ä¿æŒå¯é€‰
):
    # Step 1: Optional cleanup of old logs
    if clear_old:
        one_week_ago = datetime.utcnow() - timedelta(weeks=1)
        db.query(ApiUsageLog).filter(ApiUsageLog.called_at < one_week_ago).delete()
        db.commit()

    # Step 2: Insert detailed log (short-term log)
    log = ApiUsageLog(
        path=path,
        duration=duration,
        status_code=status_code,
        ip=ip,
        user_agent=user_agent,
        referer=referer,
        user_id=user_id,
        request_size=request_size,
        response_size=response_size,
        # å¦‚æœä¼ å…¥äº† start_timeï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ï¼Œä½¿ç”¨æ•°æ®åº“é»˜è®¤æ—¶é—´
        called_at=datetime.utcfromtimestamp(start_time) if start_time else None  # ç›´æ¥åœ¨åˆ›å»ºæ—¶å¤„ç†
    )

    # å°†æ—¥å¿—æ·»åŠ åˆ°é˜Ÿåˆ—
    log_queue.put(log)

    # Step 3: å°† ApiUsageSummary æ›´æ–°ç§»è‡³é˜Ÿåˆ—
    if user_id:
        summary_queue.put({
            'user_id': user_id,
            'path': path,
            'duration': duration,
            'request_size': request_size,
            'response_size': response_size
        })


def summary_writer():
    """æ‰¹é‡æ›´æ–° ApiUsageSummary"""
    batch = []
    batch_size = 50
    batch_timeout = 60.0

    while True:
        try:
            item = summary_queue.get(timeout=batch_timeout)

            if item is None:
                break

            batch.append(item)

            if len(batch) >= batch_size:
                _process_summary_batch(batch)
                batch = []

        except Empty:
            if batch:
                _process_summary_batch(batch)
                batch = []
        except Exception as e:
            print(f"[X] summary_writer é”™è¯¯: {e}")

    if batch:
        _process_summary_batch(batch)


def _process_summary_batch(batch: list):
    """æ‰¹é‡å¤„ç† ApiUsageSummary æ›´æ–°"""
    from app.auth.database import SessionLocal as AuthSessionLocal
    db = AuthSessionLocal()

    try:
        # æŒ‰ (user_id, path) åˆ†ç»„èšåˆ
        aggregated = {}
        for item in batch:
            key = (item['user_id'], item['path'])
            if key not in aggregated:
                aggregated[key] = {
                    'count': 0,
                    'total_duration': 0,
                    'total_upload': 0,
                    'total_download': 0
                }

            aggregated[key]['count'] += 1
            aggregated[key]['total_duration'] += item['duration']
            aggregated[key]['total_upload'] += item['request_size'] / 1024
            aggregated[key]['total_download'] += item['response_size'] / 1024

        # æ‰¹é‡æ›´æ–°
        for (user_id, path), stats in aggregated.items():
            summary = db.query(ApiUsageSummary).filter_by(
                user_id=user_id, path=path
            ).first()

            if summary:
                summary.count += stats['count']
                summary.total_duration += Decimal(round(stats['total_duration'], 2))
                summary.total_upload += Decimal(round(stats['total_upload'], 2))
                summary.total_download += Decimal(round(stats['total_download'], 2))
                summary.last_updated = datetime.utcnow()
            else:
                summary = ApiUsageSummary(
                    user_id=user_id,
                    path=path,
                    count=stats['count'],
                    total_duration=Decimal(round(stats['total_duration'], 2)),
                    total_upload=Decimal(round(stats['total_upload'], 2)),
                    total_download=Decimal(round(stats['total_download'], 2)),
                    last_updated=datetime.utcnow()
                )
                db.add(summary)

        db.commit()
        print(f"[OK] æ‰¹é‡æ›´æ–° {len(aggregated)} æ¡ ApiUsageSummary")
    except Exception as e:
        print(f"[X] ApiUsageSummary æ‰¹æ¬¡å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


async def log_detailed_api_to_db_async(
        db: Session,
        path: str,
        duration: float,
        status_code: int,
        ip: str,
        user_agent: str,
        referer: str,
        user_id: int = None,
        clear_old: bool = False,
        request_size: int = 0,
        response_size: int = 0,
        start_time: float = None
):
    """å¼‚æ­¥åŒ…è£…å™¨ï¼šå°†åŒæ­¥çš„æ•°æ®åº“æ“ä½œå¼‚æ­¥åŒ–"""
    await asyncio.to_thread(
        log_detailed_api_to_db,
        db,
        path,
        duration,
        status_code,
        ip,
        user_agent,
        referer,
        user_id,
        clear_old,
        request_size,
        response_size,
        start_time
    )


# app/service/api_logger.py
_workers_started = False
_start_lock = threading.Lock()


def start_api_logger_workers(db: Session):
    """å¯åŠ¨æ‰€æœ‰æ—¥å¿—åå°çº¿ç¨‹"""
    global _workers_started
    with _start_lock:
        if _workers_started:
            return

        # [OK] å¯åŠ¨å…³é”®è¯æ—¥å¿—å†™å…¥çº¿ç¨‹ï¼ˆlogs.dbï¼‰
        threading.Thread(target=keyword_log_writer, daemon=True).start()

        # [OK] å¯åŠ¨APIç»Ÿè®¡æ›´æ–°çº¿ç¨‹ï¼ˆlogs.dbï¼‰
        threading.Thread(target=statistics_writer, daemon=True).start()

        # [OK] å¯åŠ¨HTMLé¡µé¢è®¿é—®ç»Ÿè®¡çº¿ç¨‹ï¼ˆlogs.dbï¼‰
        threading.Thread(target=html_visit_writer, daemon=True).start()

        # [OK] å¯åŠ¨ ApiUsageLog å†™å…¥çº¿ç¨‹ï¼ˆauth.dbï¼‰
        threading.Thread(target=log_writer_thread, args=(db,), daemon=True).start()

        # [NEW] å¯åŠ¨ ApiUsageSummary æ›´æ–°çº¿ç¨‹ï¼ˆauth.dbï¼‰
        threading.Thread(target=summary_writer, daemon=True).start()

        _workers_started = True
        print("[OK] API æ—¥å¿—åå°çº¿ç¨‹å·²å¯åŠ¨")


def stop_api_logger_workers():
    """åœæ­¢æ‰€æœ‰æ—¥å¿—åå°çº¿ç¨‹"""
    try:
        keyword_log_queue.put_nowait(None)  # [OK] åœæ­¢å…³é”®è¯æ—¥å¿—çº¿ç¨‹
    except:
        pass

    try:
        statistics_queue.put_nowait(None)  # [OK] åœæ­¢ç»Ÿè®¡çº¿ç¨‹
    except:
        pass

    try:
        html_visit_queue.put_nowait(None)  # [OK] åœæ­¢HTMLè®¿é—®ç»Ÿè®¡çº¿ç¨‹
    except:
        pass

    try:
        log_queue.put_nowait(None)  # åœæ­¢ ApiUsageLog çº¿ç¨‹
    except:
        pass

    try:
        summary_queue.put_nowait(None)  # [NEW] åœæ­¢ ApiUsageSummary çº¿ç¨‹
    except:
        pass

    print("ğŸ›‘ API æ—¥å¿—åå°çº¿ç¨‹å·²åœæ­¢")


class StreamingResponseWrapper:
    """æµå¼å“åº”åŒ…è£…å™¨ - è¾¹ä¼ è¾“è¾¹ç»Ÿè®¡ï¼Œä¸ç¼“å†²æ•´ä¸ªå“åº”"""

    def __init__(self, iterator, content_type, user_role, max_size, response, on_complete_callback=None):
        self.iterator = iterator
        self.content_type = content_type
        self.user_role = user_role
        self.max_size = max_size
        self.response = response
        self.total_size = 0
        self.on_complete_callback = on_complete_callback

    async def __aiter__(self):
        """å¼‚æ­¥è¿­ä»£å™¨ - æµå¼ä¼ è¾“å“åº”"""
        try:
            async for chunk in self.iterator:
                chunk_size = len(chunk)
                self.total_size += chunk_size

                # æ¸è¿›å¼å¤§å°æ£€æŸ¥ - è¶…é™ç«‹å³ä¸­æ–­
                if self.total_size > self.max_size:
                    raise HTTPException(
                        status_code=413,
                        detail="ğŸš« ç”±æ–¼æœå‹™å™¨é™åˆ¶ï¼Œæ‚¨çš„è¿”å›æ•¸æ“šè¶…éé™åˆ¶ï¼Œè«‹æ¸›å°‘è«‹æ±‚ç¯„åœ ğŸ›‘"
                    )

                # ç›´æ¥è½¬å‘æ•°æ®ï¼Œä¸åšä»»ä½•ä¿®æ”¹
                yield chunk

        finally:
            # æµå¼ä¼ è¾“å®Œæˆåè°ƒç”¨å›è°ƒ
            if self.on_complete_callback:
                await self.on_complete_callback(self)


# è¿™é‡Œæ˜¯ä¸­é—´ä»¶ï¼Œè´Ÿè´£è®°å½•è¯·æ±‚å’Œå“åº”çš„æµé‡å¤§å°
class TrafficLoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

        # 1. å¿«é€Ÿè¿‡æ»¤
        path = request.url.path
        if any(k in path for k in IGNORE_API) or not any(k in path for k in RECORD_API):
            return await call_next(request)

        # 2. ç²å– DB Session
        # [!] è­¦å‘Šï¼šç›´æ¥ç”¨ next(get_db()) æœƒå°è‡´é€£æ¥æ´©æ¼ï¼å¿…é ˆæ‰‹å‹•é—œé–‰ã€‚
        db = next(get_db())
        user = None
        try:
            # 3. [OK] ä½¿ç”¨ await èª¿ç”¨ç•°æ­¥å‡½æ•¸
            user = await get_current_user_for_middleware(request, db=db)
        except Exception as e:
            print(f"Middleware Auth Error: {e}")
            # èªè­‰å‡ºéŒ¯ä¸æ‡‰å½±éŸ¿è«‹æ±‚ç¹¼çºŒï¼Œè¦–ç‚ºåŒ¿åç”¨æˆ¶
            user = None
        finally:
            # 4. [OK] å¿…é ˆé—œé–‰æ•¸æ“šåº«é€£æ¥ï¼é€™éå¸¸é‡è¦ï¼
            db.close()

        # 5. è®¡ç®—è¯·æ±‚å¤§å°
        cl = request.headers.get("Content-Length")

        if cl is not None and cl.isdigit():
            request_size = int(cl)
        else:
            # å¦‚æœæ˜¯ GET è¯·æ±‚ï¼Œè®¡ç®— URL å’ŒæŸ¥è¯¢å‚æ•°çš„å¤§å°
            if request.method == "GET":
                url_size = len(request.url.path) + len(request.url.query)
                request_size = url_size
            else:
                # è·å–è¯·æ±‚ä½“å¤§å°
                request_body = await request.body()
                request_size = len(request_body)

        # 6. è°ƒç”¨ä¸‹æ¸¸åº”ç”¨ï¼ˆè§†å›¾å‡½æ•°ï¼‰
        response = await call_next(request)

        # 7. ç¡®å®šç”¨æˆ·çš„å“åº”å¤§å°é™åˆ¶
        max_size = MAX_ANONYMOUS_SIZE if user is None else (
            float('inf') if user.role == "admin" else MAX_USER_SIZE
        )

        # 8. å®šä¹‰æµå¼ä¼ è¾“å®Œæˆåçš„å›è°ƒå‡½æ•°
        async def on_streaming_complete(wrapper):
            """æµå¼ä¼ è¾“å®Œæˆåè®°å½•æ—¥å¿—"""
            # è®¡ç®—å¤„ç†æ—¶é—´
            duration = time.time() - start_time

            # è·å–æœ€ç»ˆçš„å“åº”å¤§å°
            # æ³¨æ„ï¼šå§‹ç»ˆè®°å½•å‹ç¼©å‰çš„å¤§å°ï¼Œä»¥ä¿è¯ç»Ÿè®¡ä¸€è‡´æ€§
            # å‹ç¼©æ˜¯æœåŠ¡å™¨ä¼˜åŒ–æ‰‹æ®µï¼Œå¯¹ç”¨æˆ·åº”è¯¥æ˜¯é€æ˜çš„
            response_size = wrapper.total_size  # å‹ç¼©å‰çš„åŸå§‹å¤§å°

            # å¼‚æ­¥è®°å½•æ—¥å¿—
            await log_detailed_api_to_db_async(
                db=db,
                path=request.url.path,
                duration=duration,
                status_code=response.status_code,
                ip=request.client.host,
                user_agent=request.headers.get("User-Agent"),
                referer=request.headers.get("Referer"),
                user_id=user.id if user else None,
                request_size=request_size,
                response_size=response_size,
                clear_old=CLEAR_WEEK,
                start_time=start_time
            )

        # 9. ä½¿ç”¨æµå¼åŒ…è£…å™¨ï¼ˆä¸ç¼“å†²æ•´ä¸ªå“åº”ï¼‰
        wrapper = StreamingResponseWrapper(
            iterator=response.body_iterator,
            content_type=response.headers.get("Content-Type", ""),
            user_role=user.role if user else None,
            max_size=max_size,
            response=response,
            on_complete_callback=on_streaming_complete
        )

        # 10. æ›¿æ¢å“åº”è¿­ä»£å™¨
        response.body_iterator = wrapper

        return response

# ä»¥ä¸‹ä»£ç å·²åºŸå¼ƒ
# === è¯¦ç»†å“åº”è®°å½•å…¥é˜Ÿ ===
# def log_detailed_api(path, duration, status_code, ip, user_agent, referer):
#     today = datetime.now().strftime("%Y-%m-%d")
#     detailed_queue.put((path, duration, status_code, ip, user_agent, referer, today))
#
# # === åå°çº¿ç¨‹å†™å…¥è¯¦ç»†å“åº” ===
# def detailed_writer():
#     # åˆå§‹åŒ–æ•°æ®ç»“æ„
#     def init_stats():
#         return {
#             "count": 0, "total_time": 0.0, "status_codes": defaultdict(int),
#             "ips": set(), "agents": set(), "referers": set()
#         }
#
#     # ä» JSON åŠ è½½æ—§æ•°æ®
#     if os.path.exists(API_DETAILED_JSON):
#         with open(API_DETAILED_JSON, "r", encoding="utf-8") as f:
#             raw = json.load(f)
#             detailed_stats = defaultdict(init_stats, {
#                 k: {
#                     "count": v["count"],
#                     "total_time": v["total_time"],
#                     "status_codes": defaultdict(int, v["status_codes"]),
#                     "ips": set(v["ips"]),
#                     "agents": set(v["agents"]),
#                     "referers": set(v["referers"]),
#                 } for k, v in raw["detailed_stats"].items()
#             })
#             daily_stats = defaultdict(lambda: defaultdict(init_stats))
#             for date, paths in raw["daily_stats"].items():
#                 for path, v in paths.items():
#                     daily_stats[date][path] = {
#                         "count": v["count"],
#                         "total_time": v["total_time"],
#                         "status_codes": defaultdict(int, v["status_codes"]),
#                         "ips": set(v["ips"]),
#                         "agents": set(v["agents"]),
#                         "referers": set(v["referers"]),
#                     }
#     else:
#         detailed_stats = defaultdict(init_stats)
#         daily_stats = defaultdict(lambda: defaultdict(init_stats))
#
#     while True:
#         item = detailed_queue.get()
#         if item is None:
#             break
#         path, duration, status, ip, agent, referer, date = item
#
#         d = detailed_stats[path]
#         d["count"] += 1
#         d["total_time"] += duration
#         d["status_codes"][status] += 1
#         d["ips"].add(ip)
#         d["agents"].add(agent)
#         if referer:
#             d["referers"].add(referer)
#
#         d_day = daily_stats[date][path]
#         d_day["count"] += 1
#         d_day["total_time"] += duration
#         d_day["status_codes"][status] += 1
#         d_day["ips"].add(ip)
#         d_day["agents"].add(agent)
#         if referer:
#             d_day["referers"].add(referer)
#
#         # å†™å…¥ç»“æ„åŒ– JSON æ–‡ä»¶ï¼ˆæŒä¹…åŒ–ï¼‰
#         with open(API_DETAILED_JSON, "w", encoding="utf-8") as f:
#             json.dump({
#                 "detailed_stats": {
#                     k: {
#                         "count": v["count"],
#                         "total_time": v["total_time"],
#                         "status_codes": dict(v["status_codes"]),
#                         "ips": list(v["ips"]),
#                         "agents": list(v["agents"]),
#                         "referers": list(v["referers"]),
#                     } for k, v in detailed_stats.items()
#                 },
#                 "daily_stats": {
#                     date: {
#                         path: {
#                             "count": v["count"],
#                             "total_time": v["total_time"],
#                             "status_codes": dict(v["status_codes"]),
#                             "ips": list(v["ips"]),
#                             "agents": list(v["agents"]),
#                             "referers": list(v["referers"]),
#                         } for path, v in paths.items()
#                     } for date, paths in daily_stats.items()
#                 }
#             }, f, ensure_ascii=False, indent=2)
#
#         # å†™å…¥å¯è¯»æ±‡æ€»ï¼ˆå’ŒåŸæ¥ä¸€æ ·ï¼‰
#         with open(API_DETAILED_FILE, "w", encoding="utf-8") as f:
#             f.write("=== Total Summary ===\n")
#             for path, d in detailed_stats.items():
#                 avg = d["total_time"] / d["count"] if d["count"] else 0
#                 f.write(f"{path}\n  Count: {d['count']}\n  Avg Response Time: {avg:.3f}s\n")
#                 f.write(f"  Status Codes: {', '.join(f'{k}:{v}' for k, v in d['status_codes'].items())}\n")
#                 f.write("  IPs:\n" + ''.join(f"    - {ip}\n" for ip in sorted(d['ips'])))
#                 f.write("  User-Agents:\n" + ''.join(f"    - {ua}\n" for ua in sorted(d['agents'])))
#                 f.write("  Referers:\n" + ''.join(f"    - {r}\n" for r in sorted(d['referers'])))
#                 f.write("\n")
#             f.write("=== Daily Summary ===\n")
#             for date in sorted(daily_stats):
#                 f.write(f"{date}\n")
#                 for path, d in daily_stats[date].items():
#                     avg = d["total_time"] / d["count"] if d["count"] else 0
#                     f.write(f"{path}\n  Count: {d['count']}\n  Avg Response Time: {avg:.3f}s\n")
#                     f.write(f"  Status Codes: {', '.join(f'{k}:{v}' for k, v in d['status_codes'].items())}\n")
#                     f.write("  IPs:\n" + ''.join(f"    - {ip}\n" for ip in sorted(d['ips'])))
#                     f.write("  User-Agents:\n" + ''.join(f"    - {ua}\n" for ua in sorted(d['agents'])))
#                     f.write("  Referers:\n" + ''.join(f"    - {r}\n" for r in sorted(d['referers'])))
#                 f.write("\n")

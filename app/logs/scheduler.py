# logs/scheduler.py
"""
æ—¥å¿—ç³»ç»Ÿå®šæ—¶ä»»åŠ¡

åŠŸèƒ½ï¼š
1. æ¯å‘¨æ¸…ç† 30 å¤©å‰çš„æ—§æ—¥å¿—
2. æ¯å°æ—¶èšåˆå…³é”®è¯ç»Ÿè®¡
"""
import logging
from datetime import datetime, timedelta
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from sqlalchemy import and_, text

from app.logs.database import SessionLocal
from app.logs.models import ApiKeywordLog, ApiStatistics, ApiVisitLog

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = BackgroundScheduler()


def cleanup_old_logs():
    """
    æ¸…ç†æ—§æ—¥å¿—ä»»åŠ¡

    åˆ é™¤ 30 å¤©å‰çš„ï¼š
    - api_keyword_log è®°å½•
    - api_visit_log æ¯æ—¥ç»Ÿè®¡ï¼ˆä¿ç•™æ€»è®¡ï¼Œdate=NULLï¼‰
    - api_statistics æ¯æ—¥ç»Ÿè®¡ï¼ˆä¿ç•™æ€»è®¡ï¼‰
    """
    logger.info("[DEL] å¼€å§‹æ¸…ç†æ—§æ—¥å¿—...")
    db = SessionLocal()

    try:
        cutoff_date = datetime.now() - timedelta(days=365)

        # åˆ é™¤å…³é”®è¯æ—¥å¿—
        deleted_keywords = db.query(ApiKeywordLog).filter(
            ApiKeywordLog.timestamp < cutoff_date
        ).delete()

        # åˆ é™¤ HTML è®¿é—®æ¯æ—¥ç»Ÿè®¡ï¼ˆä¿ç•™æ€»è®¡ï¼‰
        deleted_visits = db.query(ApiVisitLog).filter(
            and_(
                ApiVisitLog.date.isnot(None),  # åªåˆ é™¤æ¯æ—¥ç»Ÿè®¡
                ApiVisitLog.date < cutoff_date
            )
        ).delete()

        # åˆ é™¤æ¯æ—¥ç»Ÿè®¡ï¼ˆä¿ç•™æ€»è®¡ï¼‰
        deleted_stats = db.query(ApiStatistics).filter(
            and_(
                ApiStatistics.stat_type.in_(["keyword_daily", "usage_daily"]),
                ApiStatistics.date < cutoff_date
            )
        ).delete()

        db.commit()

        logger.info(f"[OK] æ¸…ç†å®Œæˆ: åˆ é™¤äº† {deleted_keywords} æ¡å…³é”®è¯æ—¥å¿—, {deleted_visits} æ¡è®¿é—®ç»Ÿè®¡, {deleted_stats} æ¡æ¯æ—¥ç»Ÿè®¡")

    except Exception as e:
        logger.error(f"[X] æ¸…ç†æ—§æ—¥å¿—å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


def aggregate_keyword_statistics():
    """
    èšåˆå…³é”®è¯ç»Ÿè®¡ä»»åŠ¡

    é‡æ–°èšåˆï¼š
    - keyword_total: æ€»è®¡ç»Ÿè®¡
    - keyword_daily: æœ€è¿‘ 7 å¤©çš„æ¯æ—¥ç»Ÿè®¡
    """
    logger.info("[DB] å¼€å§‹èšåˆå…³é”®è¯ç»Ÿè®¡...")
    db = SessionLocal()

    try:
        # æ¸…ç©ºç°æœ‰å…³é”®è¯ç»Ÿè®¡
        db.query(ApiStatistics).filter(
            ApiStatistics.stat_type.in_(["keyword_total", "keyword_daily"])
        ).delete()

        # èšåˆæ€»è®¡
        db.execute(text("""
            INSERT INTO api_statistics (stat_type, date, category, item, count, updated_at)
            SELECT
                'keyword_total' as stat_type,
                NULL as date,
                field as category,
                value as item,
                COUNT(*) as count,
                datetime('now') as updated_at
            FROM api_keyword_log
            GROUP BY field, value
        """))

        # èšåˆæœ€è¿‘ 7 å¤©çš„æ¯æ—¥ç»Ÿè®¡
        seven_days_ago = datetime.now() - timedelta(days=7)
        db.execute(text("""
            INSERT INTO api_statistics (stat_type, date, category, item, count, updated_at)
            SELECT
                'keyword_daily' as stat_type,
                DATE(timestamp) as date,
                field as category,
                value as item,
                COUNT(*) as count,
                datetime('now') as updated_at
            FROM api_keyword_log
            WHERE timestamp >= :cutoff_date
            GROUP BY DATE(timestamp), field, value
        """), {"cutoff_date": seven_days_ago})

        db.commit()

        # ç»Ÿè®¡ç»“æœ
        from sqlalchemy import func
        keyword_total = db.query(func.count(ApiStatistics.id)).filter(
            ApiStatistics.stat_type == "keyword_total"
        ).scalar()

        keyword_daily = db.query(func.count(ApiStatistics.id)).filter(
            ApiStatistics.stat_type == "keyword_daily"
        ).scalar()

        logger.info(f"[OK] èšåˆå®Œæˆ: æ€»è®¡ {keyword_total} æ¡, æ¯æ—¥ {keyword_daily} æ¡")

    except Exception as e:
        logger.error(f"[X] èšåˆç»Ÿè®¡å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


def start_scheduler():
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    # æ¯å‘¨æ—¥å‡Œæ™¨ 3 ç‚¹æ¸…ç†æ—§æ—¥å¿—
    scheduler.add_job(
        cleanup_old_logs,
        CronTrigger(day_of_week='sun', hour=3, minute=0),
        id='cleanup_old_logs',
        name='æ¸…ç†365å¤©å‰çš„æ—§æ—¥å¿—',
        replace_existing=True
    )

    # æ¯å°æ—¶çš„ç¬¬ 5 åˆ†é’Ÿèšåˆå…³é”®è¯ç»Ÿè®¡
    scheduler.add_job(
        aggregate_keyword_statistics,
        CronTrigger(minute=5),
        id='aggregate_keyword_stats',
        name='èšåˆå…³é”®è¯ç»Ÿè®¡',
        replace_existing=True
    )

    scheduler.start()
    logger.info("[OK] å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨")
    logger.info("   - æ¸…ç†æ—§æ—¥å¿—: æ¯å‘¨æ—¥ 03:00")
    logger.info("   - èšåˆç»Ÿè®¡: æ¯å°æ—¶ç¬¬ 5 åˆ†é’Ÿ")


def stop_scheduler():
    """åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    if scheduler.running:
        scheduler.shutdown()
        logger.info("ğŸ›‘ å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")


def run_task_now(task_name: str):
    """
    ç«‹å³æ‰§è¡ŒæŒ‡å®šä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•æˆ–æ‰‹åŠ¨è§¦å‘ï¼‰

    Args:
        task_name: 'cleanup' æˆ– 'aggregate'
    """
    if task_name == 'cleanup':
        cleanup_old_logs()
    elif task_name == 'aggregate':
        aggregate_keyword_statistics()
    else:
        raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡å: {task_name}")


# å¯ä»¥ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ¥æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡
if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        task = sys.argv[1]
        run_task_now(task)
    else:
        print("ç”¨æ³•:")
        print("  python -m app.logs.scheduler cleanup    # æ¸…ç†æ—§æ—¥å¿—")
        print("  python -m app.logs.scheduler aggregate  # èšåˆç»Ÿè®¡")
